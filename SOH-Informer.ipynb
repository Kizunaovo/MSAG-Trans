{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49baab17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc7d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score  # ✅ 添加这行\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2c7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler():#对+数据进行标准化和缩放\n",
    "    def __init__(self,data):  # data.shape (N,C,L)  or (N,C)\n",
    "        self.data = data\n",
    "        if self.data.ndim == 3: # (N,C,L)其中 N 是样本数，C 是特征数，L 表示数据的长度（如时间步长）。\n",
    "            self.mean = self.data.mean(axis=(0,2), keepdims=True)#沿 axis=(0,2)（即样本维度和长度维度）计算均值、方差、最大值和最小值\n",
    "            self.var = self.data.var(axis=(0,2), keepdims=True)#reshape(1, -1, 1) 将计算结果调整为合适的形状\n",
    "            self.max = self.data.max(axis=(0,2), keepdims=True)\n",
    "            self.min = self.data.min(axis=(0,2), keepdims=True)\n",
    "        elif self.data.ndim == 2: # (N,C)\n",
    "            self.mean = self.data.mean(axis=0, keepdims=True)\n",
    "            self.var = self.data.var(axis=0, keepdims=True)#沿 axis=0（样本维度）\n",
    "            self.max = self.data.max(axis=0, keepdims=True)\n",
    "            self.min = self.data.min(axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError('data dim error!')\n",
    "        print(self.data.ndim)\n",
    "\n",
    "    def standerd(self):#标准化方法\n",
    "        X = (self.data - self.mean) / (self.var + 1e-6)\n",
    "        return X\n",
    "\n",
    "    def minmax(self,feature_range=(0,1)):#归一化方法\n",
    "        if feature_range == (0,1):\n",
    "            X = (self.data - self.min) / ((self.max - self.min) + 1e-6)\n",
    "        elif feature_range == (-1,1):\n",
    "            X = 2*(self.data - self.min) / ((self.max - self.min) + 1e-6)-1\n",
    "        else:\n",
    "            raise ValueError('feature_range error!')\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea6f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):#用于跟踪指标的实用工具，常用于深度学习模型的训练或评估过程中，帮助计算和维护例如损失或精度的平均值。\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):# 重置所有变量为初始状态\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):# 更新当前值、总和和计数器，计算新的平均值\n",
    "        self.val = val\n",
    "        self.sum += val * n \n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def eval_metrix(true_label, pred_label):\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # 强制转换为 float64 类型\n",
    "    true_label = np.array(true_label, dtype=np.float64).flatten()\n",
    "    pred_label = np.array(pred_label, dtype=np.float64).flatten()\n",
    "\n",
    "    # 异常值检查\n",
    "    if np.any(np.isnan(true_label)) or np.any(np.isnan(pred_label)):\n",
    "        raise ValueError(\"true_label or pred_label contains NaN.\")\n",
    "    if np.any(np.isinf(true_label)) or np.any(np.isinf(pred_label)):\n",
    "        raise ValueError(\"true_label or pred_label contains Inf.\")\n",
    "    if np.any(np.abs(true_label) > 1e6) or np.any(np.abs(pred_label) > 1e6):\n",
    "        raise ValueError(\"true_label or pred_label contains excessively large values.\")\n",
    "\n",
    "    # 计算指标\n",
    "    MAE = metrics.mean_absolute_error(true_label, pred_label)\n",
    "    MAPE = metrics.mean_absolute_percentage_error(true_label, pred_label)\n",
    "    MSE = metrics.mean_squared_error(true_label, pred_label)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    RE = np.sum(np.abs(true_label - pred_label)) / np.sum(np.abs(true_label) + epsilon)\n",
    "    R2 = r2_score(true_label, pred_label)\n",
    "\n",
    "    return {'MAE': MAE, 'MAPE': MAPE, 'MSE': MSE, 'RMSE': RMSE, 'RE': RE, 'R2': R2}\n",
    "\n",
    "\n",
    "        \n",
    "def monotonic_loss(pred):\n",
    "    \"\"\"\n",
    "    单调性惩罚损失：只惩罚预测值的上升趋势，鼓励 SOH 下降或平稳。\n",
    "    输入 pred: Tensor, shape (B, T) 或 (B, 1)，假设时间在 dim=1 上。\n",
    "    \"\"\"\n",
    "    if pred.dim() == 2 and pred.size(1) > 1:\n",
    "        diff = pred[:, 1:] - pred[:, :-1]\n",
    "        mono_penalty = torch.relu(diff)  # 上升部分的惩罚\n",
    "        return mono_penalty.mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "\n",
    "def save_to_txt(save_name,string):\n",
    "    f = open(save_name,mode='a')\n",
    "    f.write(string)\n",
    "    f.write('\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7453c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class XJTUDdataset():\n",
    "#     def __init__(self,args):\n",
    "#         super().__init__()\n",
    "#         self.root = 'C:/Users/86152/PycharmProjects/SOHbenchmark-main/data/XJTU'\n",
    "#         self.max_capacity = 2.0\n",
    "#         self.normalized_type = args.normalized_type\n",
    "#         self.minmax_range = args.minmax_range\n",
    "#         self.seed = args.random_seed\n",
    "#         self.batch = args.batch\n",
    "#         self.batch_size = args.batch_size\n",
    "\n",
    "\n",
    "#     def _parser_mat_data(self,battery_i_mat):# 用于解析电池数据并返回特征和标签，标签为电池的健康状态（SOH）\n",
    "#         '''\n",
    "#         :param battery_i_mat: shape:(1,len)#特定电池的原始数据矩阵（形状为 (1, len)）\n",
    "#         :return: np.array\n",
    "#         '''\n",
    "#         data = []#存放处理后的特征数据和容量标签\n",
    "#         label = []\n",
    "#         for i in range(battery_i_mat.shape[1]):#（循环次数等于数据的第二维长度）\n",
    "#             cycle_i_data = battery_i_mat[0,i]\n",
    "#             time = cycle_i_data['relative_time_min'] # (1,128)\n",
    "#             current = cycle_i_data['current_A'] # (1,128)\n",
    "#             voltage = cycle_i_data['voltage_V'] # (1,128)\n",
    "#             temperature = cycle_i_data['temperature_C'] # (1,128)\n",
    "#             capacity = cycle_i_data['capacity'][0]\n",
    "#             label.append(capacity)\n",
    "#             cycle_i = np.concatenate([time,current,voltage,temperature],axis=0)#形状为 (4, 128)\n",
    "#             data.append(cycle_i)\n",
    "#         data = np.array(data,dtype=np.float32)\n",
    "#         label = np.array(label,dtype=np.float32)\n",
    "#         print(data.shape,label.shape)\n",
    "\n",
    "#         scaler = Scaler(data)\n",
    "#         if self.normalized_type == 'standard':\n",
    "#             data = scaler.standerd()\n",
    "#         else:\n",
    "#             data = scaler.minmax(feature_range=self.minmax_range)\n",
    "#         soh = label / self.max_capacity#容量标签 label 除以电池最大容量 self.max_capacity 进行归一化。\n",
    "\n",
    "#         return data,soh\n",
    "\n",
    "#     def _encapsulation(self,train_x,train_y,test_x,test_y):\n",
    "#         '''\n",
    "#         Encapsulate the numpy.array into DataLoader\n",
    "#         :param train_x: numpy.array\n",
    "#         :param train_y: numpy.array\n",
    "#         :param test_x: numpy.array\n",
    "#         :param test_y: numpy.array\n",
    "#         :return:\n",
    "#         '''\n",
    "#         train_x = torch.from_numpy(train_x)#转换成 PyTorch 的 DataLoader 格式，方便后续的模型训练、验证和测试\n",
    "#         train_y = torch.from_numpy(train_y)\n",
    "#         test_x = torch.from_numpy(test_x)\n",
    "#         test_y = torch.from_numpy(test_y)\n",
    "\n",
    "#         train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=self.seed)# 将训练数据 train_x 和 train_y 按 80% 的比例划分为训练集和验证集，验证集的比例为 20%。random_state=self.seed 确保分割的随机性可复现\n",
    "#         train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=self.batch_size, shuffle=True,\n",
    "#                                   drop_last=False)#shuffle=True 表示打乱数据，drop_last=False 表示保留最后一个批次，即使样本数量不足一个完整批次。\n",
    "#         valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=self.batch_size, shuffle=True,\n",
    "#                                   drop_last=False)\n",
    "#         test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=self.batch_size, shuffle=False)#但 shuffle=False，保持测试数据的顺序。\n",
    "#         return train_loader, valid_loader, test_loader\n",
    "\n",
    "#     def _get_raw_data(self,path,test_battery_id):#接受参数 path（文件路径）和 test_battery_id（测试用电池 ID）。使用 loadmat 函数加载 .mat 文件数据，并将其存储在 mat 中。\n",
    "#         mat = loadmat(path)\n",
    "#         battery = mat['battery']\n",
    "#         battery_ids = list(range(1, battery.shape[1] + 1))#从 1 到电池数量编号\n",
    "#         if test_battery_id not in battery_ids:\n",
    "#             raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "\n",
    "#         test_battery = battery[0, test_battery_id - 1][0]\n",
    "#         print(f'test battery id: {test_battery_id}, test data shape: ', end='')\n",
    "#         test_x, test_y = self._parser_mat_data(test_battery)\n",
    "        \n",
    "#         train_x, train_y = [], []\n",
    "#         for id in battery_ids:\n",
    "#             if id == test_battery_id:\n",
    "#                 continue\n",
    "#             print(f'train battery id: {id}, ', end='')\n",
    "#             train_battery = battery[0, id - 1][0]\n",
    "#             x, y = self._parser_mat_data(train_battery)\n",
    "#             train_x.append(x)\n",
    "#             train_y.append(y)\n",
    "#         train_x = np.concatenate(train_x, axis=0)\n",
    "#         train_y = np.concatenate(train_y, axis=0)\n",
    "#         print('train data shape: ', train_x.shape, train_y.shape)#将 train_x 和 train_y 中的列表项按行连接成完整的训练数据矩阵和标签向量，并打印它们的形状。\n",
    "\n",
    "#         return self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "\n",
    "#     def get_charge_data(self,test_battery_id=1):\n",
    "#         print('----------- load charge data -------------')\n",
    "#         file_name = f'batch-{self.batch}.mat'\n",
    "#         self.charge_path = os.path.join(self.root, 'charge', file_name)#根据类的 batch 属性构建文件名（如 batch-1.mat），并将其与根目录 self.root 以及子文件夹 charge 组合成充电数据文件的完整路径 self.charge_path\n",
    "#         train_loader, valid_loader, test_loader = self._get_raw_data(path=self.charge_path,test_battery_id=test_battery_id)\n",
    "#         data_dict = {'train':train_loader,\n",
    "#                      'test':test_loader,\n",
    "#                      'valid':valid_loader}\n",
    "#         print('-------------  finished !  ---------------')\n",
    "#         return data_dict\n",
    "\n",
    "\n",
    "#     def get_partial_data(self,test_battery_id=1):#加载特定电压范围内的部分充电数据\n",
    "#         print('----------- load partial_charge data -------------')\n",
    "#         file_name = f'batch-{self.batch}_3.7-4.1.mat'\n",
    "#         if self.batch == 6:\n",
    "#             file_name = f'batch-{self.batch}_3.9-4.19.mat'\n",
    "#         self.partial_path = os.path.join(self.root, 'partial_charge', file_name)\n",
    "#         train_loader, valid_loader, test_loader = self._get_raw_data(path=self.partial_path,\n",
    "#                                                                      test_battery_id=test_battery_id)\n",
    "#         data_dict = {'train': train_loader,\n",
    "#                      'test': test_loader,\n",
    "#                      'valid': valid_loader}\n",
    "#         print('----------------  finished !  --------------------')\n",
    "#         return data_dict\n",
    "\n",
    "#     def _parser_xlsx(self,df_i):\n",
    "#         '''\n",
    "#         features dataframe\n",
    "#         :param df_i: shape:(N,C+1)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         N = df_i.shape[0]\n",
    "#         x = np.array(df_i.iloc[:, :-1],dtype=np.float32)\n",
    "#         label = np.array(df_i['label'],dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "#         scaler = Scaler(x)\n",
    "#         if self.normalized_type == 'standard':\n",
    "#             data = scaler.standerd()\n",
    "#         else:\n",
    "#             data = scaler.minmax(feature_range=self.minmax_range)\n",
    "#         soh = label / self.max_capacity\n",
    "\n",
    "#         return data, soh\n",
    "\n",
    "#     def get_features(self,test_battery_id=1):#加载手工特征数据，并将数据组织为训练、验证和测试集。\n",
    "#         print('----------- load features -------------')\n",
    "#         file_name = f'batch-{self.batch}_features.xlsx'\n",
    "#         self.features_path = os.path.join(self.root, 'handcraft_features', file_name)\n",
    "#         df = pd.read_excel(self.features_path,sheet_name=None)\n",
    "#         sheet_names = list(df.keys())\n",
    "#         battery_ids = list(range(1, len(sheet_names)+1))\n",
    "\n",
    "#         if test_battery_id not in battery_ids:\n",
    "#             raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "#         test_battery_df = pd.read_excel(self.features_path,sheet_name=test_battery_id-1,header=0)\n",
    "#         test_x,test_y = self._parser_xlsx(test_battery_df)\n",
    "#         print(f'test battery id: {test_battery_id}, test data shape: {test_x.shape}, {test_y.shape}')\n",
    "\n",
    "#         train_x, train_y = [], []\n",
    "        \n",
    "#         for id in battery_ids:\n",
    "#             if id == test_battery_id:\n",
    "#                 continue\n",
    "#             sheet_name = sheet_names[id-1]\n",
    "#             df_i = df[sheet_name]\n",
    "#             x, y = self._parser_xlsx(df_i)\n",
    "#             print(f'train battery id: {id}, {x.shape}, {y.shape}')\n",
    "#             train_x.append(x)\n",
    "#             train_y.append(y)\n",
    "#         train_x = np.concatenate(train_x,axis=0)\n",
    "#         train_y = np.concatenate(train_y,axis=0)\n",
    "#         print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "#         train_loader, valid_loader, test_loader = self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "#         data_dict = {'train': train_loader,\n",
    "#                      'test': test_loader,\n",
    "#                      'valid': valid_loader}\n",
    "#         print('---------------  finished !  ----------------')\n",
    "#         return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5a6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     def get_args():\n",
    "\n",
    "#         parser = argparse.ArgumentParser(description='dataloader test')\n",
    "#         parser.add_argument('--random_seed',type=int,default=2023)\n",
    "#         # data\n",
    "#         parser.add_argument('--data', type=str, default='XJTU', choices=['XJTU', 'MIT', 'CALCE'])\n",
    "#         parser.add_argument('--input_type', type=str, default='charge',\n",
    "#                             choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "#         parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "#         parser.add_argument('--minmax_range', type=tuple, default=(0, 1), choices=[(0, 1), (1, 1)])\n",
    "#         parser.add_argument('--batch_size', type=int, default=128)\n",
    "#         # the parameters for XJTU data\n",
    "#         parser.add_argument('--batch', type=int, default=1, choices=[1, 2, 3, 4, 5])\n",
    "\n",
    "#         # Use parse_known_args to ignore unrecognized arguments\n",
    "#         args, unknown = parser.parse_known_args()\n",
    "#         return args\n",
    "\n",
    "#     args = get_args()\n",
    "#     data = XJTUDdataset(args)\n",
    "#     charge_data = data.get_charge_data()\n",
    "#     partial_data = data.get_partial_data()\n",
    "#     features = data.get_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e5b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MITDdataset():\n",
    "    def __init__(self,args):\n",
    "        super(MITDdataset).__init__()\n",
    "        self.root = 'D:/pythonproject/SOHbenchmark-main/data/MIT'\n",
    "        self.max_capacity = 1.1\n",
    "        self.normalized_type = args.normalized_type\n",
    "        self.minmax_range = args.minmax_range\n",
    "        self.seed = args.random_seed\n",
    "        self.batch = args.batch\n",
    "        self.batch_size = args.batch_size\n",
    "        if args.batch < 1 or args.batch > 9:\n",
    "            raise IndexError(f'\"batch\" must be in the [1, 9], but got {self.batch}. ')\n",
    "\n",
    "\n",
    "    def _parser_mat_data(self,battery_i_mat):\n",
    "        '''\n",
    "        :param battery_i_mat: shape:(1,len)\n",
    "        :return: np.array\n",
    "        '''\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in range(battery_i_mat.shape[1]):\n",
    "            cycle_i_data = battery_i_mat[0,i]\n",
    "            time = cycle_i_data['relative_time_min'] # (1,128)\n",
    "            current = cycle_i_data['current_A'] # (1,128)\n",
    "            voltage = cycle_i_data['voltage_V'] # (1,128)\n",
    "            temperature = cycle_i_data['temperature_C'] # (1,128)\n",
    "            capacity = cycle_i_data['capacity'][0]\n",
    "            label.append(capacity)\n",
    "            cycle_i = np.concatenate([time,current,voltage,temperature],axis=0)\n",
    "            data.append(cycle_i)\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        label = np.array(label,dtype=np.float32)\n",
    "        print(data.shape,label.shape)\n",
    "\n",
    "        scaler = Scaler(data)\n",
    "        if self.normalized_type == 'standard':\n",
    "            data = scaler.standerd()\n",
    "        else:\n",
    "            data = scaler.minmax(feature_range=self.minmax_range)\n",
    "        soh = label / self.max_capacity\n",
    "\n",
    "        return data,soh\n",
    "\n",
    "    def _encapsulation(self,train_x,train_y,test_x,test_y):\n",
    "        '''\n",
    "        Encapsulate the numpy.array into DataLoader\n",
    "        :param train_x: numpy.array\n",
    "        :param train_y: numpy.array\n",
    "        :param test_x: numpy.array\n",
    "        :param test_y: numpy.array\n",
    "        :return:\n",
    "        '''\n",
    "        train_x = torch.from_numpy(train_x)\n",
    "        train_y = torch.from_numpy(train_y)\n",
    "        test_x = torch.from_numpy(test_x)\n",
    "        test_y = torch.from_numpy(test_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=self.seed)\n",
    "        train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=self.batch_size, shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=self.batch_size, shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    def _get_raw_data(self,path,test_battery_id):\n",
    "        mat = loadmat(path)\n",
    "        battery = mat['battery']\n",
    "        battery_ids = list(range(1, battery.shape[1] + 1))\n",
    "        if test_battery_id not in battery_ids:\n",
    "            raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "\n",
    "        test_battery = battery[0, test_battery_id - 1][0]\n",
    "        print(f'test battery id: {test_battery_id}, test data shape: ', end='')\n",
    "        test_x, test_y = self._parser_mat_data(test_battery)\n",
    "        train_x, train_y = [], []\n",
    "        for id in battery_ids:\n",
    "            if id == test_battery_id:\n",
    "                continue\n",
    "            print(f'train battery id: {id}, ', end='')\n",
    "            train_battery = battery[0, id - 1][0]\n",
    "            x, y = self._parser_mat_data(train_battery)\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        train_x = np.concatenate(train_x, axis=0)\n",
    "        train_y = np.concatenate(train_y, axis=0)\n",
    "        print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "        return self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "\n",
    "    def get_charge_data(self,test_battery_id=1):\n",
    "        print('----------- load charge data -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'charge'))\n",
    "        file_name = charge_files[self.batch-1]\n",
    "\n",
    "        self.charge_path = os.path.join(self.root, 'charge', file_name)\n",
    "        train_loader, valid_loader, test_loader = self._get_raw_data(path=self.charge_path,test_battery_id=test_battery_id)\n",
    "        data_dict = {'train':train_loader,\n",
    "                     'test':test_loader,\n",
    "                     'valid':valid_loader}\n",
    "        print('-------------  finished !  ---------------')\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "    def get_partial_data(self,test_battery_id=1):\n",
    "        print('----------- load partial_charge data -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'partial_charge'))\n",
    "        file_name = charge_files[self.batch - 1]\n",
    "        self.partial_path = os.path.join(self.root, 'partial_charge', file_name)\n",
    "        train_loader, valid_loader, test_loader = self._get_raw_data(path=self.partial_path,\n",
    "                                                                     test_battery_id=test_battery_id)\n",
    "        data_dict = {'train': train_loader,\n",
    "                     'test': test_loader,\n",
    "                     'valid': valid_loader}\n",
    "        print('----------------  finished !  --------------------')\n",
    "        return data_dict\n",
    "\n",
    "    def _parser_xlsx(self,df_i):\n",
    "        '''\n",
    "        features dataframe\n",
    "        :param df_i: shape:(N,C+1)\n",
    "        :return:\n",
    "        '''\n",
    "        N = df_i.shape[0]\n",
    "        x = np.array(df_i.iloc[:, :-1],dtype=np.float32)\n",
    "        label = np.array(df_i['label'],dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        scaler = Scaler(x)\n",
    "        if self.normalized_type == 'standard':\n",
    "            data = scaler.standerd()\n",
    "        else:\n",
    "            data = scaler.minmax(feature_range=self.minmax_range)\n",
    "        soh = label / self.max_capacity\n",
    "\n",
    "        return data, soh\n",
    "\n",
    "    def get_features(self,test_battery_id=1):\n",
    "        print('----------- load features -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'handcraft_features'))\n",
    "        file_name = charge_files[self.batch - 1]\n",
    "        self.features_path = os.path.join(self.root, 'handcraft_features', file_name)\n",
    "        df = pd.read_excel(self.features_path,sheet_name=None)\n",
    "        sheet_names = list(df.keys())\n",
    "        battery_ids = list(range(1, len(sheet_names)+1))\n",
    "\n",
    "        if test_battery_id not in battery_ids:\n",
    "            raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "        test_battery_df = pd.read_excel(self.features_path,sheet_name=test_battery_id-1,header=0)\n",
    "        test_x,test_y = self._parser_xlsx(test_battery_df)\n",
    "        print(f'test battery id: {test_battery_id}, test data shape: {test_x.shape}, {test_y.shape}')\n",
    "\n",
    "        train_x, train_y = [], []\n",
    "        for id in battery_ids:\n",
    "            if id == test_battery_id:\n",
    "                continue\n",
    "            sheet_name = sheet_names[id-1]\n",
    "            df_i = df[sheet_name]\n",
    "            x, y = self._parser_xlsx(df_i)\n",
    "            print(f'train battery id: {id}, {x.shape}, {y.shape}')\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        train_x = np.concatenate(train_x,axis=0)\n",
    "        train_y = np.concatenate(train_y,axis=0)\n",
    "        print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "        train_loader, valid_loader, test_loader = self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "        data_dict = {'train': train_loader,\n",
    "                     'test': test_loader,\n",
    "                     'valid': valid_loader}\n",
    "        print('---------------  finished !  ----------------')\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67791ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- load charge data -------------\n",
      "test battery id: 3, test data shape: (989, 4, 128) (989, 1)\n",
      "3\n",
      "train battery id: 1, (914, 4, 128) (914, 1)\n",
      "3\n",
      "train battery id: 2, (678, 4, 128) (678, 1)\n",
      "3\n",
      "train battery id: 4, (1089, 4, 128) (1089, 1)\n",
      "3\n",
      "train battery id: 5, (751, 4, 128) (751, 1)\n",
      "3\n",
      "train data shape:  (3432, 4, 128) (3432, 1)\n",
      "-------------  finished !  ---------------\n",
      "----------- load partial_charge data -------------\n",
      "test battery id: 5, test data shape: (751, 4, 128) (751, 1)\n",
      "3\n",
      "train battery id: 1, (914, 4, 128) (914, 1)\n",
      "3\n",
      "train battery id: 2, (678, 4, 128) (678, 1)\n",
      "3\n",
      "train battery id: 3, (989, 4, 128) (989, 1)\n",
      "3\n",
      "train battery id: 4, (1089, 4, 128) (1089, 1)\n",
      "3\n",
      "train data shape:  (3670, 4, 128) (3670, 1)\n",
      "----------------  finished !  --------------------\n",
      "----------- load features -------------\n",
      "2\n",
      "test battery id: 2, test data shape: (678, 67), (678, 1)\n",
      "2\n",
      "train battery id: 1, (914, 67), (914, 1)\n",
      "2\n",
      "train battery id: 3, (989, 67), (989, 1)\n",
      "2\n",
      "train battery id: 4, (1089, 67), (1089, 1)\n",
      "2\n",
      "train battery id: 5, (751, 67), (751, 1)\n",
      "train data shape:  (3743, 67) (3743, 1)\n",
      "---------------  finished !  ----------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    def get_args():\n",
    "\n",
    "        parser = argparse.ArgumentParser(description='dataloader test')\n",
    "        parser.add_argument('--random_seed',type=int,default=2023)\n",
    "        # data\n",
    "        parser.add_argument('--data', type=str, default='MIT', choices=['XJTU', 'MIT'])\n",
    "        parser.add_argument('--input_type', type=str, default='charge', choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "        parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "        parser.add_argument('--minmax_range', type=tuple, default=(-1, 1), choices=[(0, 1), (-1, 1)])\n",
    "        parser.add_argument('--batch_size', type=int, default=64)\n",
    "        # the parameters for XJTU data\n",
    "        parser.add_argument('--batch', type=int, default=2, choices=[1, 2, 3, 4, 5])\n",
    "\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        return args\n",
    "\n",
    "\n",
    "    args = get_args()\n",
    "    data = MITDdataset(args)\n",
    "    charge_data = data.get_charge_data(test_battery_id=3)\n",
    "    partial_charge = data.get_partial_data(test_battery_id=5)\n",
    "    features = data.get_features(test_battery_id=2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10ce749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c69f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResBlock(nn.Module):#ResBlock 是一个残差模块，通常用于深度神经网络中，通过在卷积路径之外添加“跳跃连接”或“残差连接”来帮助信号从输入直接传递到输出。这种结构可以有效地缓解梯度消失问题，提升深层网络的训练效果\n",
    "\n",
    "#     def __init__(self, input_channel, output_channel, stride):\n",
    "#         super(ResBlock, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv1d(input_channel, output_channel, kernel_size=3, stride=stride, padding=1),\n",
    "#             nn.BatchNorm1d(output_channel),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Conv1d(output_channel, output_channel, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm1d(output_channel)\n",
    "#         )\n",
    "\n",
    "#         self.skip_connection = nn.Sequential()\n",
    "#         if output_channel != input_channel:#如果输入通道和输出通道不相等，或步幅不为1，跳跃连接包含一个调整通道数的卷积层和一个批量归一化层，以使维度匹配。\n",
    "#             self.skip_connection = nn.Sequential(\n",
    "#                 nn.Conv1d(input_channel, output_channel, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm1d(output_channel)\n",
    "#             )\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):#前向传播过程\n",
    "#         out = self.conv(x)#输入 x 先通过卷积路径 self.conv。\n",
    "#         out = self.skip_connection(x) + out#输入 x 通过跳跃连接，与卷积路径的输出相加。\n",
    "#         out = self.relu(out)#相加后的结果通过 ReLU 激活函数输出，得到模块的输出。\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "#     def __init__(self):\n",
    "#         super(CNN,self).__init__()#旨在处理输入形状为 (N, 4, 128) 的数据，其中 N 是批量大小，4 是通道数，128 是每个通道的长度。\n",
    "#         self.layer1 = ResBlock(input_channel=4, output_channel=16, stride=1) # N,16,128\n",
    "#         self.layer2 = ResBlock(input_channel=16, output_channel=32, stride=2) # N,32,64\n",
    "#         self.layer3 = ResBlock(input_channel=32, output_channel=64, stride=2)  # N,64,32\n",
    "#         self.layer4 = ResBlock(input_channel=64, output_channel=96, stride=2)  # N,96,16\n",
    "#         self.layer5 = ResBlock(input_channel=96, output_channel=128, stride=2)  # N,128,8\n",
    "\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128 * 8, 128),#预测器将卷积层的输出展平，输入维度为 128 * 8 = 1024， 使用一个全连接层将其映射到大小为 128 的隐藏层\n",
    "#             nn.ReLU(),#用 ReLU 激活，最终映射到输出维度 1\n",
    "#             nn.Linear(128, 1)#输出 pred 为标量预测结果（形状为 (N, 1)），适用于回归任务。\n",
    "\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         '''\n",
    "#         :param x: shape:(N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = self.layer5(out)\n",
    "#         pred = self.predictor(out.view(out.size(0), -1))#在最后一个卷积层输出后，将数据展平，并通过 predictor 进行回归预测。\n",
    "#         return pred#输出 pred：为形状 (N, 1) 的张量，通常用作回归输出。\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)#生成了一个形状为 (30, 4, 128) 的随机张量，表示一个批量大小为30的输入数据，每个样本有4个通道、128个时间步或特征\n",
    "\n",
    "#     net = CNN()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)#输入形状：x.shape 应为 (30, 4, 128)。输出形状：根据模型设计，y.shape 应为 (30, 1)，即批量大小为30的标量预测。\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())#计算 CNN 模型的总参数量，并打印。\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0927d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.net = nn.LSTM(input_size=4,hidden_size=128,num_layers=2,batch_first=True)\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64,1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         :param x: (N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         x = x.transpose(1, 2)\n",
    "#         embed,(_,_) = self.net(x)\n",
    "#         out = embed[:,-1,:]\n",
    "#         pred = self.predictor(out)\n",
    "\n",
    "#         return pred\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)\n",
    "\n",
    "#     net = LSTM()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac418e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(128*4,256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256,128),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         :param x: (N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         x = x.view(-1,4*128)\n",
    "#         fea = self.net(x)\n",
    "#         out = self.predictor(fea)\n",
    "#         return out\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)\n",
    "\n",
    "#     net = MLP()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9212d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model, kernel_size=3, padding=1)\n",
    "        nn.init.kaiming_normal_(self.tokenConv.weight, mode='fan_in')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.value_embedding = TokenEmbedding(c_in, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, scale=None, dropout=0.1):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        B, L, H, D = queries.shape\n",
    "        scale = self.scale or 1. / math.sqrt(D)\n",
    "\n",
    "        scores = torch.einsum(\"blhd,bshd->bhls\", queries, keys)\n",
    "        scores = scale * scores\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        context = torch.einsum(\"bhls,bshd->blhd\", attn, values)\n",
    "        return context.contiguous()\n",
    "\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.attn = ProbAttention(dropout=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L, _ = x.shape\n",
    "        Q = self.q_proj(x).view(B, L, self.n_heads, self.d_k)\n",
    "        K = self.k_proj(x).view(B, L, self.n_heads, self.d_k)\n",
    "        V = self.v_proj(x).view(B, L, self.n_heads, self.d_k)\n",
    "\n",
    "        context = self.attn(Q, K, V)\n",
    "        context = context.view(B, L, self.d_model)\n",
    "        return self.out_proj(context)\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = AttentionLayer(d_model, n_heads, dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attn(self.norm1(x)))\n",
    "        x = x + self.dropout(self.ffn(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, enc_in=4, d_model=64, n_heads=4, e_layers=2, d_ff=128, out_len=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = DataEmbedding(enc_in, d_model, dropout)\n",
    "        self.encoder = nn.Sequential(\n",
    "            *[EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(e_layers)]\n",
    "        )\n",
    "        self.projection = nn.Linear(d_model, out_len)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C] or [B, C, T]\n",
    "        # 自动处理输入维度，确保进入 embedding 前是 [B, T, C]\n",
    "        if x.size(1) == self.embedding.value_embedding.tokenConv.in_channels and x.size(2) != self.embedding.value_embedding.tokenConv.in_channels:\n",
    "            # 如果输入是 [B, C, T]，则转为 [B, T, C]\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.embedding(x)        # [B, T, d_model]\n",
    "        x = self.encoder(x)          # [B, T, d_model]\n",
    "        out = self.projection(x[:, -1, :])  # 取最后一个时间步作为输出\n",
    "        return out  # [B, out_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ec840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa3c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed54fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e705ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe29c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf90393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca102f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import rand\n",
    "class SOHMode(nn.Module):\n",
    "    '''\n",
    "    data shape:\n",
    "    charge_data (N,4,128)\n",
    "    partial_data (N,4,128)\n",
    "    features (N,1,67)\n",
    "    '''\n",
    "    #args (Namespace): 配置参数，包括设备（device）、学习率（lr）和权重衰减（weight_decay）等。\n",
    "    def __init__(self,args):\n",
    "        super(SOHMode,self).__init__()\n",
    "        self.args = args  # 保存配置参数\n",
    "        self.pre_net = self._preprocessing_net()  # 预处理网络，用于特征提取或预处理\n",
    "        self.backbone = self._backbone()  # 主干网络，用于模型的主要预测逻辑\n",
    "        self.pre_net.to(args.device)  # 将预处理网络放到指定设备\n",
    "        self.backbone.to(args.device)  # 将主干网络放到指定设备\n",
    "        # 初始化模型权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=self.args.lr, weight_decay=self.args.weight_decay)\n",
    "\n",
    "        # 学习率调度器\n",
    "        #self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer,milestones=[30, 70], gamma=0.5 )\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=10, factor=0.5)\n",
    "       # 损失函数\n",
    "       #self.mse = torch.nn.MSELoss()\n",
    "        self.criterion = torch.nn.SmoothL1Loss()  # 替换 MSELoss 更鲁棒\n",
    "        self.lambda_mono = 0.5\n",
    "\n",
    "        # 存储损失的平均值\n",
    "        self.loss_meter = AverageMeter()\n",
    "        # 损失分项记录\n",
    "        self.loss_smoothl1_list = []\n",
    "        self.loss_monotonic_list = []\n",
    "\n",
    "        # 存储最佳状态\n",
    "        self.best_state = None\n",
    "        \n",
    "        # ======== 标签归一化参数（建议） ========\n",
    "        self.label_mean = args.label_mean if hasattr(args, 'label_mean') else 0.8\n",
    "        self.label_std = args.label_std if hasattr(args, 'label_std') else 0.1\n",
    "\n",
    "    def _preprocessing_net(self):\n",
    "        '''\n",
    "        A preprocess network which transform data from different sources into the same shape\n",
    "        :return: A network, with output shape (N,4,128)\n",
    "        '''\n",
    "        if self.args.input_type == 'charge': # charge_data (N,4,128)\n",
    "            net = nn.Conv1d(in_channels=4,out_channels=4,kernel_size=1)\n",
    "        elif self.args.input_type == 'partial_charge': #partial_data (N,4,128)\n",
    "            net = nn.Conv1d(in_channels=4, out_channels=4, kernel_size=1)\n",
    "        else:  # features (N,1,67)\n",
    "            net = nn.Linear(67,128*4) #(N,128*4)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    def _backbone(self):\n",
    "        if self.args.model == 'Informer':\n",
    "            return Informer(\n",
    "                enc_in=self.args.enc_in,\n",
    "                d_model=64,\n",
    "                n_heads=4,\n",
    "                e_layers=2,\n",
    "                d_ff=128,\n",
    "                out_len=self.args.c_out,\n",
    "                dropout=0.1\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {self.args.model}\")\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):#前向传播函数，根据输入类型调用预处理网络并将其输入到主干网络中。\n",
    "        if self.args.input_type == 'handcraft_features':  # 仅在特征输入时应用预处理\n",
    "            x = self.pre_net(x)  # 使用全连接层将特征转换\n",
    "            x = x.view(-1, 4, 128)  # 重塑张量形状\n",
    "        out = self.backbone(x)  # 使用主干网络进行预测\n",
    "        return out\n",
    "\n",
    "    def _train_one_epoch(self, train_loader, e):\n",
    "        # 将预处理网络和主干网络设为训练模式\n",
    "        self.pre_net.train()\n",
    "        self.backbone.train()\n",
    "\n",
    "        # 重置损失监测器\n",
    "        self.loss_meter.reset()\n",
    "\n",
    "        # 遍历训练数据\n",
    "        for data, label in train_loader:\n",
    "            # 将数据和标签加载到指定设备上\n",
    "            data, label = data.to(self.args.device), label.to(self.args.device)\n",
    "        \n",
    "            # 标签归一化（训练阶段）\n",
    "            label_norm = (label - self.label_mean) / self.label_std\n",
    "           \n",
    "            pred = self.forward(data)\n",
    "            \n",
    "            loss_main = self.criterion(pred, label_norm)\n",
    "            loss_mono = monotonic_loss(pred)  # ← 无论如何都先定义\n",
    "\n",
    "            if e <= 5:\n",
    "                loss = loss_main  # 前几轮不要加入单调性\n",
    "            else:\n",
    "                loss = loss_main + self.lambda_mono * loss_mono\n",
    "\n",
    "            # 梯度清零、反向传播并更新参数\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)  # 防止梯度爆炸\n",
    "            self.optimizer.step()\n",
    "  \n",
    "            # 更新损失监测器\n",
    "            self.loss_meter.update(loss.item())\n",
    "            self.loss_smoothl1_list.append(loss_main.item())\n",
    "            self.loss_monotonic_list.append(loss_mono.item())\n",
    "\n",
    "\n",
    "    def predict(self,test_loader):\n",
    "        # 将预处理网络和主干网络设为评估模式\n",
    "        self.pre_net.eval()\n",
    "        self.backbone.eval()\n",
    "        self.loss_meter.reset()\n",
    "        \n",
    "        # 重置损失监测器 \n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        \n",
    "        # 禁用梯度计算，以提高推理效率\n",
    "        with torch.no_grad():\n",
    "            # 遍历测试数据\n",
    "            for data, label in test_loader:\n",
    "                 # 将数据和标签加载到指定设备上\n",
    "                data, label = data.to(self.args.device), label.to(self.args.device)\n",
    "                # 前向传播，获取预测结果\n",
    "                pred = self.forward(data)\n",
    "                \n",
    "                 # 反归一化预测值\n",
    "                pred = pred * self.label_std + self.label_mean\n",
    "                \n",
    "                loss = self.criterion((pred - self.label_mean) / self.label_std, (label - self.label_mean) / self.label_std)\n",
    "\n",
    "                # 更新损失监测器\n",
    "                self.loss_meter.update(loss.item())\n",
    "                # 转换为 NumPy 并 flatten，避免嵌套列表结构\n",
    "                true_label.append(label.cpu().numpy().astype(np.float64).reshape(-1))\n",
    "                pred_label.append(pred.cpu().numpy().astype(np.float64).reshape(-1))\n",
    "\n",
    "            # 拼接为完整数组\n",
    "        self.true_label = np.concatenate(true_label)\n",
    "        self.pred_label = np.concatenate(pred_label)\n",
    "\n",
    "        return self.true_label, self.pred_label\n",
    "\n",
    "    def Train(self,train_loader,valid_loader,test_loader,save_folder=None):\n",
    "        min_loss = 10 #记录验证集的最低损失，初始值设为10。\n",
    "        stop = 0 #记录早停计数器，如果训练损失未改善超过一定轮数，则停止训练。\n",
    "        self.train_loss = []#列表分别存储每个 epoch 的训练和验证损失\n",
    "        self.valid_loss = []#用于保存测试集的真实和预测标签\n",
    "        \n",
    "        # 新增指标保存列表\n",
    "        self.valid_MAE = []\n",
    "        self.valid_MAPE = []\n",
    "        self.valid_R2 = []\n",
    "    \n",
    "        \n",
    "        self.test_loss = []\n",
    "        self.true_label, self.pred_label = None, None\n",
    "        \n",
    "        \n",
    "        self.best_MAE = float('inf')\n",
    "        self.best_MAPE = float('inf')\n",
    "        self.best_R2 = -float('inf')\n",
    "        \n",
    "        print(\"valid size:\", len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "\n",
    "        for e in range(1,self.args.n_epoch+1):\n",
    "            self._train_one_epoch(train_loader, e)\n",
    "            train_l = self.loss_meter.avg\n",
    "            self.train_loss.append(train_l)\n",
    "            \n",
    "            stop += 1\n",
    "            #计算验证集的预测和损失\n",
    "            self.predict(valid_loader)\n",
    "            valid_l = self.loss_meter.avg\n",
    "            self.valid_loss.append(valid_l)\n",
    "            self.scheduler.step(valid_l)#更新学习率\n",
    "            # 计算额外指标\n",
    "            errors = eval_metrix(self.true_label, self.pred_label)\n",
    "            self.valid_MAE.append(errors['MAE'])\n",
    "            self.valid_MAPE.append(errors['MAPE'])\n",
    "            self.valid_R2.append(errors['R2'])\n",
    "            \n",
    "            #从优化器中提取学习率并打印训练信息（当前 epoch、训练损失、验证损失和学习率）\n",
    "            lr = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            \n",
    "            print(f\"epoch=[{e}/{self.args.n_epoch}]  train loss : {train_l:.7f}  valid loss : {valid_l:.7f}  lr : {lr:.7f}\")\n",
    "            \n",
    "            if e % 10 == 0:\n",
    "                print(\"\")\n",
    "\n",
    "            #如果当前验证损失 valid_l 低于 min_loss，保存当前模型状态（pre_net 和 backbone）并计算测试集的预测损失。\n",
    "            if valid_l < min_loss:\n",
    "                self.best_state = {'pre_net': self.pre_net.state_dict(),\n",
    "                                   'backbone': self.backbone.state_dict()}\n",
    "                self.true_label, self.pred_label = self.predict(test_loader)\n",
    "                errors = eval_metrix(self.true_label, self.pred_label)\n",
    "\n",
    "                # 保存最佳指标（★）\n",
    "                if errors['MAE'] < self.best_MAE:\n",
    "                    self.best_MAE = errors['MAE']\n",
    "                if errors['MAPE'] < self.best_MAPE:\n",
    "                    self.best_MAPE = errors['MAPE']\n",
    "                if errors['R2'] > self.best_R2:\n",
    "                    self.best_R2 = errors['R2']\n",
    "\n",
    "\n",
    "                print(f' ------ test loss : {self.loss_meter.avg:.7f}')\n",
    "                for k, v in errors.items():\n",
    "                    print(f\"{k} : {v:.7f}\")\n",
    "\n",
    "                min_loss = valid_l\n",
    "                stop = 0\n",
    "           \n",
    "        if save_folder is not None:\n",
    "            self.save_all(save_folder)#调用 save_all(save_folder) 保存模型和训练结果。\n",
    "             # Plot the loss after training\n",
    "            self._plot_loss(self.train_loss, self.valid_loss)\n",
    "            true_label, pred_label = self.predict(test_loader)\n",
    "            \n",
    "            self._plot_metrics(self.valid_MAE, self.valid_MAPE, self.valid_R2)\n",
    "            self.plot_soh_predictions(self.true_label, self.pred_label)\n",
    "            self.plot_soh_predictions(self.true_label, self.pred_label)\n",
    "            \n",
    "\n",
    "\n",
    "    def save_all(self,folder):#保存模型的训练结果和最佳模型的状态\n",
    "        if not os.path.exists(folder):#如果指定的 folder 路径不存在，则创建该文件夹以存储模型数据和结果。\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        prefix = self.args.model + '_' + self.args.input_type#以模型名称和输入类型为前缀，方便文件区分\n",
    "        errors = eval_metrix(self.true_label,self.pred_label)#利用 eval_metrix 函数将真实标签和预测标签进行比较得到误差\n",
    "        np.savez(os.path.join(folder, f'{prefix}_results.npz'),\n",
    "            train_loss=np.array(self.train_loss),\n",
    "            valid_loss=np.array(self.valid_loss),\n",
    "            valid_MAE=np.array(self.valid_MAE),\n",
    "            valid_MAPE=np.array(self.valid_MAPE),\n",
    "            valid_R2=np.array(self.valid_R2),\n",
    "            true_label=np.array(self.true_label),\n",
    "            pred_label=np.array(self.pred_label),\n",
    "            test_errors=np.array(errors),\n",
    "            best_MAE=np.array(self.best_MAE),\n",
    "            best_MAPE=np.array(self.best_MAPE),\n",
    "            best_R2=np.array(self.best_R2),\n",
    "\n",
    "            loss_smoothl1=np.array(self.loss_smoothl1_list),\n",
    "            loss_monotonic=np.array(self.loss_monotonic_list),\n",
    "            lambda_mono=np.array(self.lambda_mono)\n",
    "        )\n",
    "        torch.save(self.best_state, os.path.join(folder, f'{prefix}_model.pkl'))\n",
    "\n",
    "    def _plot_loss(self,train_loss,valid_loss):#绘制训练和验证集的损失变化趋势\n",
    "\n",
    "        self.fig_loss = plt.figure()\n",
    "        plt.plot(train_loss,label='train')\n",
    "        plt.plot(valid_loss,label='valid')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()#显示训练损失（train）和验证损失（valid）的标签，方便辨识\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def _plot_metrics(self, MAE_list, MAPE_list, R2_list):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(MAE_list, label='MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(MAPE_list, label='MAPE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('Validation MAPE')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(R2_list, label='R2 Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('R2')\n",
    "        plt.title('Validation R2 Score')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def plot_soh_predictions(self,true_label, pred_label):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(true_label, label='True SOH', color='blue', linewidth=2)\n",
    "        plt.plot(pred_label, label='Predicted SOH', color='orange', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.title('True vs Predicted SOH Values')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('SOH Value')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def _initialize_weights(self):#用于为模型的各层权重进行初始化。权重初始化是深度学习模型训练中非常重要的一步，能够帮助模型更好、更快地收敛。\n",
    "        for m in self.modules():#通过 self.modules() 遍历模型的所有子模块（层），根据每个子模块的类型来选择合适的初始化方式。\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')#使用 kaiming_normal_ 进行权重初始化，这是一种适合ReLU激活函数的初始化方法，有助于防止梯度消失或爆炸。\n",
    "                if m.bias is not None:#如果卷积层有偏置（bias），则将其初始化为 0。\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)#将批归一化层的权重 weight 初始化为1，保持其默认缩放。\n",
    "                nn.init.constant_(m.bias, 0)#将偏置 bias 初始化为0，确保批归一化层的均值偏置初始为零偏移。\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)#使用 nn.init.normal_ 方法将线性层的权重 weight 初始化为均值为0、标准差为0.01的正态分布。\n",
    "                nn.init.constant_(m.bias, 0)#将偏置 bias 初始化为0，以确保线性层的初始输出没有偏移。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b663eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informer(\n",
      "  (embedding): DataEmbedding(\n",
      "    (value_embedding): TokenEmbedding(\n",
      "      (tokenConv): Conv1d(4, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    )\n",
      "    (position_embedding): PositionalEmbedding()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (0): EncoderLayer(\n",
      "      (attn): AttentionLayer(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (attn): ProbAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (attn): AttentionLayer(\n",
      "        (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (attn): ProbAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (ffn): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (1): GELU(approximate='none')\n",
      "        (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (projection): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Input shape: torch.Size([30, 128, 4])\n",
      "Output shape: torch.Size([30, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def get_args():\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description='A benchmark for SOH estimation')\n",
    "        parser.add_argument('--random_seed', type=int, default=2023)\n",
    "        # data\n",
    "        parser.add_argument('--data', type=str, default='MIT', choices=['XJTU', 'MIT'])\n",
    "        parser.add_argument('--input_type', type=str, default='charge',\n",
    "                            choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "        parser.add_argument('--batch_size', type=int, default=64)\n",
    "        parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "        parser.add_argument('--minmax_range', type=tuple, default=(-1, 1), choices=[(0, 1), (-1, 1)])\n",
    "        parser.add_argument('--batch', type=int, default=9, choices=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "        # model\n",
    "        parser.add_argument('--model', type=str, default='Informer', choices=['Informer', 'TransCNN','CNN', 'LSTM', 'GRU', 'MLP', 'Attention'])\n",
    "\n",
    "        parser.add_argument('--lr', type=float, default=2e-3)\n",
    "        parser.add_argument('--weight_decay', default=5e-4)\n",
    "        parser.add_argument('--n_epoch', type=int, default=150)\n",
    "        parser.add_argument('--early_stop', default=20)\n",
    "        parser.add_argument('--device', default='cpu')\n",
    "        \n",
    "        # Informer 专用参数（你当前的报错来自它）\n",
    "        parser.add_argument('--enc_in', type=int, default=4)       # 输入维度\n",
    "        parser.add_argument('--dec_in', type=int, default=4)\n",
    "        parser.add_argument('--c_out', type=int, default=1)\n",
    "        parser.add_argument('--seq_len', type=int, default=128)    # 输入序列长度\n",
    "        parser.add_argument('--label_len', type=int, default=64)   # 编码长度\n",
    "        parser.add_argument('--out_len', type=int, default=1)      # 输出序列长度\n",
    "\n",
    "        # Use parse_known_args to ignore unrecognized arguments\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        args.label_mean = 0.8\n",
    "        args.label_std = 0.1\n",
    "        return args\n",
    "\n",
    "\n",
    "    args = get_args()\n",
    "    model = SOHMode(args)\n",
    "    #用于模型的随机输入张量，模拟实际的电池数据输入\n",
    "    x1 = torch.rand(30,4,128).to('cpu') #形状为 (30, 4, 128) 的张量，表示批次大小为 30，每个样本有 4 个特征通道，特征长度为 128。\n",
    "    x2 = torch.rand(30,1,67) #形状为 (30, 1, 67) 的张量，用于可能的附加特征。\n",
    "    enc_in = 4      # 输入通道数\n",
    "    d_model = 64    # Transformer特征维度\n",
    "    out_len = 1     # 预测输出长度\n",
    "\n",
    "    model = Informer(enc_in=enc_in, d_model=d_model, out_len=out_len)\n",
    "    print(model)\n",
    "\n",
    "    x = torch.rand(30, 128, enc_in)  # batch=30, seq_len=128, channels=4\n",
    "    print(\"Input shape:\", x.shape)\n",
    "\n",
    "    y = model(x)\n",
    "    print(\"Output shape:\", y.shape)  # 预期 torch.Size([30, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91c36f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CrossVal] Batch=1, Fold=1, Model=Informer, Input=charge, Norm=minmax, Exp=1\n",
      "----------- load charge data -------------\n",
      "test battery id: 1, test data shape: (761, 4, 128) (761, 1)\n",
      "3\n",
      "train battery id: 2, (743, 4, 128) (743, 1)\n",
      "3\n",
      "train battery id: 3, (907, 4, 128) (907, 1)\n",
      "3\n",
      "train battery id: 4, (748, 4, 128) (748, 1)\n",
      "3\n",
      "train battery id: 5, (616, 4, 128) (616, 1)\n",
      "3\n",
      "train data shape:  (3014, 4, 128) (3014, 1)\n",
      "-------------  finished !  ---------------\n",
      "valid size: 603\n",
      "epoch=[1/100]  train loss : 0.1312136  valid loss : 0.0551902  lr : 0.0020000\n",
      " ------ test loss : 0.0698091\n",
      "MAE : 0.0275198\n",
      "MAPE : 0.0310125\n",
      "MSE : 0.0013312\n",
      "RMSE : 0.0364862\n",
      "RE : 0.0300184\n",
      "R2 : 0.1088865\n",
      "epoch=[2/100]  train loss : 0.0445828  valid loss : 0.0227572  lr : 0.0020000\n",
      " ------ test loss : 0.0427077\n",
      "MAE : 0.0207482\n",
      "MAPE : 0.0229073\n",
      "MSE : 0.0008361\n",
      "RMSE : 0.0289155\n",
      "RE : 0.0226319\n",
      "R2 : 0.4403243\n",
      "epoch=[3/100]  train loss : 0.0167776  valid loss : 0.0115442  lr : 0.0020000\n",
      " ------ test loss : 0.0179619\n",
      "MAE : 0.0098000\n",
      "MAPE : 0.0111572\n",
      "MSE : 0.0003334\n",
      "RMSE : 0.0182592\n",
      "RE : 0.0106897\n",
      "R2 : 0.7768272\n",
      "epoch=[4/100]  train loss : 0.0138673  valid loss : 0.0078432  lr : 0.0020000\n",
      " ------ test loss : 0.0194059\n",
      "MAE : 0.0084989\n",
      "MAPE : 0.0096630\n",
      "MSE : 0.0003603\n",
      "RMSE : 0.0189822\n",
      "RE : 0.0092705\n",
      "R2 : 0.7588055\n",
      "epoch=[5/100]  train loss : 0.0113679  valid loss : 0.0080219  lr : 0.0020000\n",
      "epoch=[6/100]  train loss : 0.0101390  valid loss : 0.0099932  lr : 0.0020000\n",
      "epoch=[7/100]  train loss : 0.0091209  valid loss : 0.0040162  lr : 0.0020000\n",
      " ------ test loss : 0.0087549\n",
      "MAE : 0.0060655\n",
      "MAPE : 0.0069343\n",
      "MSE : 0.0001594\n",
      "RMSE : 0.0126238\n",
      "RE : 0.0066162\n",
      "R2 : 0.8933272\n",
      "epoch=[8/100]  train loss : 0.0084322  valid loss : 0.0048545  lr : 0.0020000\n",
      "epoch=[9/100]  train loss : 0.0071023  valid loss : 0.0045988  lr : 0.0020000\n",
      "epoch=[10/100]  train loss : 0.0057629  valid loss : 0.0041109  lr : 0.0020000\n",
      "\n",
      "epoch=[11/100]  train loss : 0.0065383  valid loss : 0.0030974  lr : 0.0020000\n",
      " ------ test loss : 0.0048197\n",
      "MAE : 0.0059517\n",
      "MAPE : 0.0067834\n",
      "MSE : 0.0000915\n",
      "RMSE : 0.0095658\n",
      "RE : 0.0064921\n",
      "R2 : 0.9387478\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m     multi_task_MIT()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossval_mit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[43mcrossval_mit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcharge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mminmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 143\u001b[0m, in \u001b[0;36mcrossval_mit\u001b[1;34m(models, input_types, normalization_types, test_ids, batches, experiment_num)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[CrossVal] Batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Norm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Exp=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/crossval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_copy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/batch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiment\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 143\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 57\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(args, save_prefix)\u001b[0m\n\u001b[0;32m     55\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m load_data(args, test_battery_id\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtest_battery_id)\n\u001b[0;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m SOHMode(args)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model, data_loader\n\u001b[0;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[14], line 181\u001b[0m, in \u001b[0;36mSOHMode.Train\u001b[1;34m(self, train_loader, valid_loader, test_loader, save_folder)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(valid_loader\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     train_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_meter\u001b[38;5;241m.\u001b[39mavg\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(train_l)\n",
      "Cell \u001b[1;32mIn[14], line 98\u001b[0m, in \u001b[0;36mSOHMode._train_one_epoch\u001b[1;34m(self, train_loader, e)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# 标签归一化（训练阶段）\u001b[39;00m\n\u001b[0;32m     96\u001b[0m label_norm \u001b[38;5;241m=\u001b[39m (label \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_mean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_std\n\u001b[1;32m---> 98\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m loss_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(pred, label_norm)\n\u001b[0;32m    101\u001b[0m loss_mono \u001b[38;5;241m=\u001b[39m monotonic_loss(pred)  \u001b[38;5;66;03m# ← 无论如何都先定义\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 79\u001b[0m, in \u001b[0;36mSOHMode.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_net(x)  \u001b[38;5;66;03m# 使用全连接层将特征转换\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m128\u001b[39m)  \u001b[38;5;66;03m# 重塑张量形状\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 使用主干网络进行预测\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 124\u001b[0m, in \u001b[0;36mInformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    121\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    123\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)        \u001b[38;5;66;03m# [B, T, d_model]\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# [B, T, d_model]\u001b[39;00m\n\u001b[0;32m    125\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(x[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# 取最后一个时间步作为输出\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 103\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)))\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 84\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     81\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(x)\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[0;32m     82\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(x)\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m---> 84\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m context \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(context)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 57\u001b[0m, in \u001b[0;36mProbAttention.forward\u001b[1;34m(self, queries, keys, values)\u001b[0m\n\u001b[0;32m     55\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblhd,bshd->bhls\u001b[39m\u001b[38;5;124m\"\u001b[39m, queries, keys)\n\u001b[0;32m     56\u001b[0m scores \u001b[38;5;241m=\u001b[39m scale \u001b[38;5;241m*\u001b[39m scores\n\u001b[1;32m---> 57\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn)\n\u001b[0;32m     60\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhls,bshd->blhd\u001b[39m\u001b[38;5;124m\"\u001b[39m, attn, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='A benchmark for SOH estimation')\n",
    "    parser.add_argument('--random_seed', type=int, default=2023)\n",
    "    # data\n",
    "    parser.add_argument('--data', type=str, default='MIT',choices=['XJTU','MIT'])\n",
    "    parser.add_argument('--input_type',type=str,default='charge',choices=['charge','partial_charge','handcraft_features'])\n",
    "    parser.add_argument('--test_battery_id',type=int,default=1,help='test battery id, 1-8 for XJTU (1-15 for batch-2), 1-5 for MIT')\n",
    "    parser.add_argument('--batch_size',type=int,default=64)\n",
    "    parser.add_argument('--normalized_type',type=str,default='minmax',choices=['minmax','standard'])\n",
    "    parser.add_argument('--minmax_range',type=tuple,default=(-1,1),choices=[(0,1),(-1,1)])\n",
    "    parser.add_argument('--batch', type=int, default=9,choices=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "    # model\n",
    "    parser.add_argument('--model',type=str,default='Informer', choices=['Informer', 'TransCNN','CNN','LSTM','GRU','MLP','Attention'])\n",
    "    # CNN lr=2e-3  early_stop=30\n",
    "\n",
    "    parser.add_argument('--lr',type=float,default=2e-3)\n",
    "    parser.add_argument('--weight_decay', default=5e-4)\n",
    "    parser.add_argument('--n_epoch',type=int,default=100)\n",
    "    parser.add_argument('--early_stop',default=30)\n",
    "    parser.add_argument('--device',default='cpu')\n",
    "    parser.add_argument('--save_folder',default='results')\n",
    "    parser.add_argument('--experiment_num',default=1,type=int,help='The number of times you want to repeat the same experiment')\n",
    "    parser.add_argument('--test_ids', type=int, nargs='+', default=[1], help='List of test battery IDs to evaluate')\n",
    "    \n",
    "    # Informer 专用参数（你当前的报错来自它）\n",
    "    parser.add_argument('--enc_in', type=int, default=4)       # 输入维度\n",
    "    parser.add_argument('--dec_in', type=int, default=4)\n",
    "    parser.add_argument('--c_out', type=int, default=1)\n",
    "    parser.add_argument('--seq_len', type=int, default=128)    # 输入序列长度\n",
    "    parser.add_argument('--label_len', type=int, default=64)   # 编码长度\n",
    "    parser.add_argument('--out_len', type=int, default=1)      # 输出序列长度\n",
    "\n",
    "    # Use parse_known_args to ignore unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def load_data(args, test_battery_id):\n",
    "    loader = XJTUDdataset(args) if args.data == 'XJTU' else MITDdataset(args)\n",
    "    if args.input_type == 'charge':\n",
    "        return loader.get_charge_data(test_battery_id)\n",
    "    elif args.input_type == 'partial_charge':\n",
    "        return loader.get_partial_data(test_battery_id)\n",
    "    else:\n",
    "        return loader.get_features(test_battery_id)\n",
    "\n",
    "def run_experiment(args, save_prefix):\n",
    "    try:\n",
    "        data_loader = load_data(args, test_battery_id=args.test_battery_id)\n",
    "        model = SOHMode(args)\n",
    "        model.Train(data_loader['train'], data_loader['valid'], data_loader['test'],\n",
    "                    save_folder=save_prefix)\n",
    "        del model, data_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as ex:\n",
    "        print(f\"[Error] Batch {args.batch}, TestBattery {args.test_battery_id}, Model {args.model}, Input {args.input_type}: {ex}\")\n",
    "        traceback.print_exc()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    \n",
    "def main(args, batches=None, test_ids=None):\n",
    "    batches = batches or [args.batch]\n",
    "    test_ids = test_ids or [args.test_battery_id]\n",
    "\n",
    "    for batch in batches:\n",
    "        for test_id in test_ids:\n",
    "            args.batch = batch\n",
    "            args.test_battery_id = test_id\n",
    "            for e in range(args.experiment_num):\n",
    "                print(f\"\\nRunning: {args.model}, Batch={batch}, TestID={test_id}, Exp={e + 1}\")\n",
    "                save_path = f'results/{args.data}-{args.input_type}/{args.model}/batch{batch}-testbattery{test_id}/experiment{e + 1}'\n",
    "                run_experiment(args, save_path)\n",
    "\n",
    "\n",
    "def multi_task_XJTU():  # 一次性训练所有模型和所有输入类型\n",
    "    args = get_args()\n",
    "    setattr(args,'data','XJTU')\n",
    "    for m in ['Informer', 'TransCNN','CNN','MLP','Attention','LSTM','GRU']:\n",
    "        for type in ['handcraft_features','charge','partial_charge']:\n",
    "            setattr(args, 'model', m)\n",
    "            setattr(args, 'input_type',type)\n",
    "            main(args)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def multi_task_MIT():\n",
    "    args = get_args()\n",
    "    for norm in ['standard', 'minmax']:\n",
    "        args.normalized_type = norm\n",
    "        args.minmax_range = (0, 1)\n",
    "        args.data = 'MIT'\n",
    "        for m in ['GRU']:\n",
    "            for input_type in ['partial_charge']:\n",
    "                args.model = m\n",
    "                args.input_type = input_type\n",
    "                for batch in range(1, 10):\n",
    "                    args.batch = batch\n",
    "                    for test_id in [1, 2, 3, 4, 5]:\n",
    "                        args.test_battery_id = test_id\n",
    "                        for e in range(5):\n",
    "                            print(f\"\\n[MIT Run] Model={args.model}, Batch={batch}, TestID={test_id}, Input={input_type}, Norm={norm}, Exp={e + 1}\")\n",
    "                            save_path = f'results/{norm}/{args.data}-{args.input_type}/{args.model}/batch{batch}-testbattery{test_id}/experiment{e + 1}'\n",
    "                            run_experiment(args, save_path)\n",
    "\n",
    "\n",
    "def crossval_mit(\n",
    "    models=['Informer'], \n",
    "    input_types=['charge', 'partial_charge', 'handcraft_features'],\n",
    "    normalization_types=['minmax', 'standard'],\n",
    "    test_ids=[1, 2, 3, 4, 5],\n",
    "    batches=range(1, 10),\n",
    "    experiment_num=1\n",
    "):\n",
    "    args = get_args()\n",
    "    args.data = 'MIT'\n",
    "    args.experiment_num = experiment_num\n",
    "\n",
    "    for norm in normalization_types:\n",
    "        args.normalized_type = norm\n",
    "        args.minmax_range = (-1, 1) if norm == 'minmax' else (0, 1)\n",
    "\n",
    "        for batch in batches:\n",
    "            for test_id in test_ids:\n",
    "                for model_name in models:\n",
    "                    for input_type in input_types:\n",
    "                        for e in range(experiment_num):\n",
    "                            args_copy = deepcopy(args)\n",
    "                            args_copy.batch = batch\n",
    "                            args_copy.test_battery_id = test_id\n",
    "                            args_copy.model = model_name\n",
    "                            args_copy.input_type = input_type\n",
    "\n",
    "                            print(f\"\\n[CrossVal] Batch={batch}, Fold={test_id}, Model={model_name}, \"\n",
    "                                  f\"Input={input_type}, Norm={norm}, Exp={e+1}\")\n",
    "                            \n",
    "                            save_path = f'results/crossval/{norm}/{args_copy.data}-{input_type}/{model_name}/batch{batch}-fold{test_id}/experiment{e+1}'\n",
    "                            run_experiment(args_copy, save_path)\n",
    "\n",
    "                          \n",
    "                            \n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "#     from argparse import ArgumentParse\n",
    "\n",
    "#     parser = ArgumentParser()\n",
    "#     parser.add_argument('--task', type=str, default='crossval_mit', choices=[\n",
    "#         'single', 'multi_xjtu', 'multi_mit', 'crossval_mit'\n",
    "#     ])\n",
    "#     task_args, _ = parser.parse_known_args()\n",
    "    task = 'crossval_mit'\n",
    "\n",
    "    if task == 'single':\n",
    "        for e in range(args.experiment_num):\n",
    "            print(f\"\\n[Single Run] {args.model} on Battery {args.test_battery_id}, Batch {args.batch}, Input={args.input_type}\")\n",
    "            save_path = f'results/{args.data}-{args.input_type}/{args.model}/batch{args.batch}-testbattery{args.test_battery_id}/experiment{e + 1}'\n",
    "            run_experiment(args, save_path)\n",
    "\n",
    "    elif task == 'multi_xjtu':\n",
    "        multi_task_XJTU()\n",
    "\n",
    "    elif task == 'multi_mit':\n",
    "        multi_task_MIT()\n",
    "\n",
    "    elif task == 'crossval_mit':\n",
    "        crossval_mit(\n",
    "        models=['Informer'],\n",
    "        input_types=['charge'],\n",
    "        normalization_types=['minmax'],\n",
    "        test_ids=[1],\n",
    "        batches=[1],\n",
    "        experiment_num=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672569d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
