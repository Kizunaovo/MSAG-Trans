{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49baab17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc7d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score  # ✅ 添加这行\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2c7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler():#对+数据进行标准化和缩放\n",
    "    def __init__(self,data):  # data.shape (N,C,L)  or (N,C)\n",
    "        self.data = data\n",
    "        if self.data.ndim == 3: # (N,C,L)其中 N 是样本数，C 是特征数，L 表示数据的长度（如时间步长）。\n",
    "            self.mean = self.data.mean(axis=(0,2), keepdims=True)#沿 axis=(0,2)（即样本维度和长度维度）计算均值、方差、最大值和最小值\n",
    "            self.var = self.data.var(axis=(0,2), keepdims=True)#reshape(1, -1, 1) 将计算结果调整为合适的形状\n",
    "            self.max = self.data.max(axis=(0,2), keepdims=True)\n",
    "            self.min = self.data.min(axis=(0,2), keepdims=True)\n",
    "        elif self.data.ndim == 2: # (N,C)\n",
    "            self.mean = self.data.mean(axis=0, keepdims=True)\n",
    "            self.var = self.data.var(axis=0, keepdims=True)#沿 axis=0（样本维度）\n",
    "            self.max = self.data.max(axis=0, keepdims=True)\n",
    "            self.min = self.data.min(axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError('data dim error!')\n",
    "        print(self.data.ndim)\n",
    "\n",
    "    def standerd(self):#标准化方法\n",
    "        X = (self.data - self.mean) / (self.var + 1e-6)\n",
    "        return X\n",
    "\n",
    "    def minmax(self,feature_range=(0,1)):#归一化方法\n",
    "        if feature_range == (0,1):\n",
    "            X = (self.data - self.min) / ((self.max - self.min) + 1e-6)\n",
    "        elif feature_range == (-1,1):\n",
    "            X = 2*(self.data - self.min) / ((self.max - self.min) + 1e-6)-1\n",
    "        else:\n",
    "            raise ValueError('feature_range error!')\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea6f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):#用于跟踪指标的实用工具，常用于深度学习模型的训练或评估过程中，帮助计算和维护例如损失或精度的平均值。\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):# 重置所有变量为初始状态\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):# 更新当前值、总和和计数器，计算新的平均值\n",
    "        self.val = val\n",
    "        self.sum += val * n \n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def eval_metrix(true_label, pred_label):\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # 强制转换为 float64 类型\n",
    "    true_label = np.array(true_label, dtype=np.float64).flatten()\n",
    "    pred_label = np.array(pred_label, dtype=np.float64).flatten()\n",
    "\n",
    "    # 异常值检查\n",
    "    if np.any(np.isnan(true_label)) or np.any(np.isnan(pred_label)):\n",
    "        raise ValueError(\"true_label or pred_label contains NaN.\")\n",
    "    if np.any(np.isinf(true_label)) or np.any(np.isinf(pred_label)):\n",
    "        raise ValueError(\"true_label or pred_label contains Inf.\")\n",
    "    if np.any(np.abs(true_label) > 1e6) or np.any(np.abs(pred_label) > 1e6):\n",
    "        raise ValueError(\"true_label or pred_label contains excessively large values.\")\n",
    "\n",
    "    # 计算指标\n",
    "    MAE = metrics.mean_absolute_error(true_label, pred_label)\n",
    "    MAPE = metrics.mean_absolute_percentage_error(true_label, pred_label)\n",
    "    MSE = metrics.mean_squared_error(true_label, pred_label)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    RE = np.sum(np.abs(true_label - pred_label)) / np.sum(np.abs(true_label) + epsilon)\n",
    "    R2 = r2_score(true_label, pred_label)\n",
    "\n",
    "    return {'MAE': MAE, 'MAPE': MAPE, 'MSE': MSE, 'RMSE': RMSE, 'RE': RE, 'R2': R2}\n",
    "\n",
    "\n",
    "        \n",
    "def monotonic_loss(pred):\n",
    "    \"\"\"\n",
    "    单调性惩罚损失：只惩罚预测值的上升趋势，鼓励 SOH 下降或平稳。\n",
    "    输入 pred: Tensor, shape (B, T) 或 (B, 1)，假设时间在 dim=1 上。\n",
    "    \"\"\"\n",
    "    if pred.dim() == 2 and pred.size(1) > 1:\n",
    "        diff = pred[:, 1:] - pred[:, :-1]\n",
    "        mono_penalty = torch.relu(diff)  # 上升部分的惩罚\n",
    "        return mono_penalty.mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "\n",
    "def save_to_txt(save_name,string):\n",
    "    f = open(save_name,mode='a')\n",
    "    f.write(string)\n",
    "    f.write('\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7453c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class XJTUDdataset():\n",
    "#     def __init__(self,args):\n",
    "#         super().__init__()\n",
    "#         self.root = 'C:/Users/86152/PycharmProjects/SOHbenchmark-main/data/XJTU'\n",
    "#         self.max_capacity = 2.0\n",
    "#         self.normalized_type = args.normalized_type\n",
    "#         self.minmax_range = args.minmax_range\n",
    "#         self.seed = args.random_seed\n",
    "#         self.batch = args.batch\n",
    "#         self.batch_size = args.batch_size\n",
    "\n",
    "\n",
    "#     def _parser_mat_data(self,battery_i_mat):# 用于解析电池数据并返回特征和标签，标签为电池的健康状态（SOH）\n",
    "#         '''\n",
    "#         :param battery_i_mat: shape:(1,len)#特定电池的原始数据矩阵（形状为 (1, len)）\n",
    "#         :return: np.array\n",
    "#         '''\n",
    "#         data = []#存放处理后的特征数据和容量标签\n",
    "#         label = []\n",
    "#         for i in range(battery_i_mat.shape[1]):#（循环次数等于数据的第二维长度）\n",
    "#             cycle_i_data = battery_i_mat[0,i]\n",
    "#             time = cycle_i_data['relative_time_min'] # (1,128)\n",
    "#             current = cycle_i_data['current_A'] # (1,128)\n",
    "#             voltage = cycle_i_data['voltage_V'] # (1,128)\n",
    "#             temperature = cycle_i_data['temperature_C'] # (1,128)\n",
    "#             capacity = cycle_i_data['capacity'][0]\n",
    "#             label.append(capacity)\n",
    "#             cycle_i = np.concatenate([time,current,voltage,temperature],axis=0)#形状为 (4, 128)\n",
    "#             data.append(cycle_i)\n",
    "#         data = np.array(data,dtype=np.float32)\n",
    "#         label = np.array(label,dtype=np.float32)\n",
    "#         print(data.shape,label.shape)\n",
    "\n",
    "#         scaler = Scaler(data)\n",
    "#         if self.normalized_type == 'standard':\n",
    "#             data = scaler.standerd()\n",
    "#         else:\n",
    "#             data = scaler.minmax(feature_range=self.minmax_range)\n",
    "#         soh = label / self.max_capacity#容量标签 label 除以电池最大容量 self.max_capacity 进行归一化。\n",
    "\n",
    "#         return data,soh\n",
    "\n",
    "#     def _encapsulation(self,train_x,train_y,test_x,test_y):\n",
    "#         '''\n",
    "#         Encapsulate the numpy.array into DataLoader\n",
    "#         :param train_x: numpy.array\n",
    "#         :param train_y: numpy.array\n",
    "#         :param test_x: numpy.array\n",
    "#         :param test_y: numpy.array\n",
    "#         :return:\n",
    "#         '''\n",
    "#         train_x = torch.from_numpy(train_x)#转换成 PyTorch 的 DataLoader 格式，方便后续的模型训练、验证和测试\n",
    "#         train_y = torch.from_numpy(train_y)\n",
    "#         test_x = torch.from_numpy(test_x)\n",
    "#         test_y = torch.from_numpy(test_y)\n",
    "\n",
    "#         train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=self.seed)# 将训练数据 train_x 和 train_y 按 80% 的比例划分为训练集和验证集，验证集的比例为 20%。random_state=self.seed 确保分割的随机性可复现\n",
    "#         train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=self.batch_size, shuffle=True,\n",
    "#                                   drop_last=False)#shuffle=True 表示打乱数据，drop_last=False 表示保留最后一个批次，即使样本数量不足一个完整批次。\n",
    "#         valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=self.batch_size, shuffle=True,\n",
    "#                                   drop_last=False)\n",
    "#         test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=self.batch_size, shuffle=False)#但 shuffle=False，保持测试数据的顺序。\n",
    "#         return train_loader, valid_loader, test_loader\n",
    "\n",
    "#     def _get_raw_data(self,path,test_battery_id):#接受参数 path（文件路径）和 test_battery_id（测试用电池 ID）。使用 loadmat 函数加载 .mat 文件数据，并将其存储在 mat 中。\n",
    "#         mat = loadmat(path)\n",
    "#         battery = mat['battery']\n",
    "#         battery_ids = list(range(1, battery.shape[1] + 1))#从 1 到电池数量编号\n",
    "#         if test_battery_id not in battery_ids:\n",
    "#             raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "\n",
    "#         test_battery = battery[0, test_battery_id - 1][0]\n",
    "#         print(f'test battery id: {test_battery_id}, test data shape: ', end='')\n",
    "#         test_x, test_y = self._parser_mat_data(test_battery)\n",
    "        \n",
    "#         train_x, train_y = [], []\n",
    "#         for id in battery_ids:\n",
    "#             if id == test_battery_id:\n",
    "#                 continue\n",
    "#             print(f'train battery id: {id}, ', end='')\n",
    "#             train_battery = battery[0, id - 1][0]\n",
    "#             x, y = self._parser_mat_data(train_battery)\n",
    "#             train_x.append(x)\n",
    "#             train_y.append(y)\n",
    "#         train_x = np.concatenate(train_x, axis=0)\n",
    "#         train_y = np.concatenate(train_y, axis=0)\n",
    "#         print('train data shape: ', train_x.shape, train_y.shape)#将 train_x 和 train_y 中的列表项按行连接成完整的训练数据矩阵和标签向量，并打印它们的形状。\n",
    "\n",
    "#         return self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "\n",
    "#     def get_charge_data(self,test_battery_id=1):\n",
    "#         print('----------- load charge data -------------')\n",
    "#         file_name = f'batch-{self.batch}.mat'\n",
    "#         self.charge_path = os.path.join(self.root, 'charge', file_name)#根据类的 batch 属性构建文件名（如 batch-1.mat），并将其与根目录 self.root 以及子文件夹 charge 组合成充电数据文件的完整路径 self.charge_path\n",
    "#         train_loader, valid_loader, test_loader = self._get_raw_data(path=self.charge_path,test_battery_id=test_battery_id)\n",
    "#         data_dict = {'train':train_loader,\n",
    "#                      'test':test_loader,\n",
    "#                      'valid':valid_loader}\n",
    "#         print('-------------  finished !  ---------------')\n",
    "#         return data_dict\n",
    "\n",
    "\n",
    "#     def get_partial_data(self,test_battery_id=1):#加载特定电压范围内的部分充电数据\n",
    "#         print('----------- load partial_charge data -------------')\n",
    "#         file_name = f'batch-{self.batch}_3.7-4.1.mat'\n",
    "#         if self.batch == 6:\n",
    "#             file_name = f'batch-{self.batch}_3.9-4.19.mat'\n",
    "#         self.partial_path = os.path.join(self.root, 'partial_charge', file_name)\n",
    "#         train_loader, valid_loader, test_loader = self._get_raw_data(path=self.partial_path,\n",
    "#                                                                      test_battery_id=test_battery_id)\n",
    "#         data_dict = {'train': train_loader,\n",
    "#                      'test': test_loader,\n",
    "#                      'valid': valid_loader}\n",
    "#         print('----------------  finished !  --------------------')\n",
    "#         return data_dict\n",
    "\n",
    "#     def _parser_xlsx(self,df_i):\n",
    "#         '''\n",
    "#         features dataframe\n",
    "#         :param df_i: shape:(N,C+1)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         N = df_i.shape[0]\n",
    "#         x = np.array(df_i.iloc[:, :-1],dtype=np.float32)\n",
    "#         label = np.array(df_i['label'],dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "#         scaler = Scaler(x)\n",
    "#         if self.normalized_type == 'standard':\n",
    "#             data = scaler.standerd()\n",
    "#         else:\n",
    "#             data = scaler.minmax(feature_range=self.minmax_range)\n",
    "#         soh = label / self.max_capacity\n",
    "\n",
    "#         return data, soh\n",
    "\n",
    "#     def get_features(self,test_battery_id=1):#加载手工特征数据，并将数据组织为训练、验证和测试集。\n",
    "#         print('----------- load features -------------')\n",
    "#         file_name = f'batch-{self.batch}_features.xlsx'\n",
    "#         self.features_path = os.path.join(self.root, 'handcraft_features', file_name)\n",
    "#         df = pd.read_excel(self.features_path,sheet_name=None)\n",
    "#         sheet_names = list(df.keys())\n",
    "#         battery_ids = list(range(1, len(sheet_names)+1))\n",
    "\n",
    "#         if test_battery_id not in battery_ids:\n",
    "#             raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "#         test_battery_df = pd.read_excel(self.features_path,sheet_name=test_battery_id-1,header=0)\n",
    "#         test_x,test_y = self._parser_xlsx(test_battery_df)\n",
    "#         print(f'test battery id: {test_battery_id}, test data shape: {test_x.shape}, {test_y.shape}')\n",
    "\n",
    "#         train_x, train_y = [], []\n",
    "        \n",
    "#         for id in battery_ids:\n",
    "#             if id == test_battery_id:\n",
    "#                 continue\n",
    "#             sheet_name = sheet_names[id-1]\n",
    "#             df_i = df[sheet_name]\n",
    "#             x, y = self._parser_xlsx(df_i)\n",
    "#             print(f'train battery id: {id}, {x.shape}, {y.shape}')\n",
    "#             train_x.append(x)\n",
    "#             train_y.append(y)\n",
    "#         train_x = np.concatenate(train_x,axis=0)\n",
    "#         train_y = np.concatenate(train_y,axis=0)\n",
    "#         print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "#         train_loader, valid_loader, test_loader = self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "#         data_dict = {'train': train_loader,\n",
    "#                      'test': test_loader,\n",
    "#                      'valid': valid_loader}\n",
    "#         print('---------------  finished !  ----------------')\n",
    "#         return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5a6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     def get_args():\n",
    "\n",
    "#         parser = argparse.ArgumentParser(description='dataloader test')\n",
    "#         parser.add_argument('--random_seed',type=int,default=2023)\n",
    "#         # data\n",
    "#         parser.add_argument('--data', type=str, default='XJTU', choices=['XJTU', 'MIT', 'CALCE'])\n",
    "#         parser.add_argument('--input_type', type=str, default='charge',\n",
    "#                             choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "#         parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "#         parser.add_argument('--minmax_range', type=tuple, default=(0, 1), choices=[(0, 1), (1, 1)])\n",
    "#         parser.add_argument('--batch_size', type=int, default=128)\n",
    "#         # the parameters for XJTU data\n",
    "#         parser.add_argument('--batch', type=int, default=1, choices=[1, 2, 3, 4, 5])\n",
    "\n",
    "#         # Use parse_known_args to ignore unrecognized arguments\n",
    "#         args, unknown = parser.parse_known_args()\n",
    "#         return args\n",
    "\n",
    "#     args = get_args()\n",
    "#     data = XJTUDdataset(args)\n",
    "#     charge_data = data.get_charge_data()\n",
    "#     partial_data = data.get_partial_data()\n",
    "#     features = data.get_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e5b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MITDdataset():\n",
    "    def __init__(self,args):\n",
    "        super(MITDdataset).__init__()\n",
    "        self.root = 'D:/pythonproject/SOHbenchmark-main/data/MIT'\n",
    "        self.max_capacity = 1.1\n",
    "        self.normalized_type = args.normalized_type\n",
    "        self.minmax_range = args.minmax_range\n",
    "        self.seed = args.random_seed\n",
    "        self.batch = args.batch\n",
    "        self.batch_size = args.batch_size\n",
    "        if args.batch < 1 or args.batch > 9:\n",
    "            raise IndexError(f'\"batch\" must be in the [1, 9], but got {self.batch}. ')\n",
    "\n",
    "\n",
    "    def _parser_mat_data(self,battery_i_mat):\n",
    "        '''\n",
    "        :param battery_i_mat: shape:(1,len)\n",
    "        :return: np.array\n",
    "        '''\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in range(battery_i_mat.shape[1]):\n",
    "            cycle_i_data = battery_i_mat[0,i]\n",
    "            time = cycle_i_data['relative_time_min'] # (1,128)\n",
    "            current = cycle_i_data['current_A'] # (1,128)\n",
    "            voltage = cycle_i_data['voltage_V'] # (1,128)\n",
    "            temperature = cycle_i_data['temperature_C'] # (1,128)\n",
    "            capacity = cycle_i_data['capacity'][0]\n",
    "            label.append(capacity)\n",
    "            cycle_i = np.concatenate([time,current,voltage,temperature],axis=0)\n",
    "            data.append(cycle_i)\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        label = np.array(label,dtype=np.float32)\n",
    "        print(data.shape,label.shape)\n",
    "\n",
    "        scaler = Scaler(data)\n",
    "        if self.normalized_type == 'standard':\n",
    "            data = scaler.standerd()\n",
    "        else:\n",
    "            data = scaler.minmax(feature_range=self.minmax_range)\n",
    "        soh = label / self.max_capacity\n",
    "\n",
    "        return data,soh\n",
    "\n",
    "    def _encapsulation(self,train_x,train_y,test_x,test_y):\n",
    "        '''\n",
    "        Encapsulate the numpy.array into DataLoader\n",
    "        :param train_x: numpy.array\n",
    "        :param train_y: numpy.array\n",
    "        :param test_x: numpy.array\n",
    "        :param test_y: numpy.array\n",
    "        :return:\n",
    "        '''\n",
    "        train_x = torch.from_numpy(train_x)\n",
    "        train_y = torch.from_numpy(train_y)\n",
    "        test_x = torch.from_numpy(test_x)\n",
    "        test_y = torch.from_numpy(test_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=self.seed)\n",
    "        train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=self.batch_size, shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=self.batch_size, shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    def _get_raw_data(self,path,test_battery_id):\n",
    "        mat = loadmat(path)\n",
    "        battery = mat['battery']\n",
    "        battery_ids = list(range(1, battery.shape[1] + 1))\n",
    "        if test_battery_id not in battery_ids:\n",
    "            raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "\n",
    "        test_battery = battery[0, test_battery_id - 1][0]\n",
    "        print(f'test battery id: {test_battery_id}, test data shape: ', end='')\n",
    "        test_x, test_y = self._parser_mat_data(test_battery)\n",
    "        train_x, train_y = [], []\n",
    "        for id in battery_ids:\n",
    "            if id == test_battery_id:\n",
    "                continue\n",
    "            print(f'train battery id: {id}, ', end='')\n",
    "            train_battery = battery[0, id - 1][0]\n",
    "            x, y = self._parser_mat_data(train_battery)\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        train_x = np.concatenate(train_x, axis=0)\n",
    "        train_y = np.concatenate(train_y, axis=0)\n",
    "        print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "        return self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "\n",
    "    def get_charge_data(self,test_battery_id=1):\n",
    "        print('----------- load charge data -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'charge'))\n",
    "        file_name = charge_files[self.batch-1]\n",
    "\n",
    "        self.charge_path = os.path.join(self.root, 'charge', file_name)\n",
    "        train_loader, valid_loader, test_loader = self._get_raw_data(path=self.charge_path,test_battery_id=test_battery_id)\n",
    "        data_dict = {'train':train_loader,\n",
    "                     'test':test_loader,\n",
    "                     'valid':valid_loader}\n",
    "        print('-------------  finished !  ---------------')\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "    def get_partial_data(self,test_battery_id=1):\n",
    "        print('----------- load partial_charge data -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'partial_charge'))\n",
    "        file_name = charge_files[self.batch - 1]\n",
    "        self.partial_path = os.path.join(self.root, 'partial_charge', file_name)\n",
    "        train_loader, valid_loader, test_loader = self._get_raw_data(path=self.partial_path,\n",
    "                                                                     test_battery_id=test_battery_id)\n",
    "        data_dict = {'train': train_loader,\n",
    "                     'test': test_loader,\n",
    "                     'valid': valid_loader}\n",
    "        print('----------------  finished !  --------------------')\n",
    "        return data_dict\n",
    "\n",
    "    def _parser_xlsx(self,df_i):\n",
    "        '''\n",
    "        features dataframe\n",
    "        :param df_i: shape:(N,C+1)\n",
    "        :return:\n",
    "        '''\n",
    "        N = df_i.shape[0]\n",
    "        x = np.array(df_i.iloc[:, :-1],dtype=np.float32)\n",
    "        label = np.array(df_i['label'],dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        scaler = Scaler(x)\n",
    "        if self.normalized_type == 'standard':\n",
    "            data = scaler.standerd()\n",
    "        else:\n",
    "            data = scaler.minmax(feature_range=self.minmax_range)\n",
    "        soh = label / self.max_capacity\n",
    "\n",
    "        return data, soh\n",
    "\n",
    "    def get_features(self,test_battery_id=1):\n",
    "        print('----------- load features -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'handcraft_features'))\n",
    "        file_name = charge_files[self.batch - 1]\n",
    "        self.features_path = os.path.join(self.root, 'handcraft_features', file_name)\n",
    "        df = pd.read_excel(self.features_path,sheet_name=None)\n",
    "        sheet_names = list(df.keys())\n",
    "        battery_ids = list(range(1, len(sheet_names)+1))\n",
    "\n",
    "        if test_battery_id not in battery_ids:\n",
    "            raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "        test_battery_df = pd.read_excel(self.features_path,sheet_name=test_battery_id-1,header=0)\n",
    "        test_x,test_y = self._parser_xlsx(test_battery_df)\n",
    "        print(f'test battery id: {test_battery_id}, test data shape: {test_x.shape}, {test_y.shape}')\n",
    "\n",
    "        train_x, train_y = [], []\n",
    "        for id in battery_ids:\n",
    "            if id == test_battery_id:\n",
    "                continue\n",
    "            sheet_name = sheet_names[id-1]\n",
    "            df_i = df[sheet_name]\n",
    "            x, y = self._parser_xlsx(df_i)\n",
    "            print(f'train battery id: {id}, {x.shape}, {y.shape}')\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        train_x = np.concatenate(train_x,axis=0)\n",
    "        train_y = np.concatenate(train_y,axis=0)\n",
    "        print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "        train_loader, valid_loader, test_loader = self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "        data_dict = {'train': train_loader,\n",
    "                     'test': test_loader,\n",
    "                     'valid': valid_loader}\n",
    "        print('---------------  finished !  ----------------')\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67791ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- load charge data -------------\n",
      "test battery id: 3, test data shape: (989, 4, 128) (989, 1)\n",
      "3\n",
      "train battery id: 1, (914, 4, 128) (914, 1)\n",
      "3\n",
      "train battery id: 2, (678, 4, 128) (678, 1)\n",
      "3\n",
      "train battery id: 4, (1089, 4, 128) (1089, 1)\n",
      "3\n",
      "train battery id: 5, (751, 4, 128) (751, 1)\n",
      "3\n",
      "train data shape:  (3432, 4, 128) (3432, 1)\n",
      "-------------  finished !  ---------------\n",
      "----------- load partial_charge data -------------\n",
      "test battery id: 5, test data shape: (751, 4, 128) (751, 1)\n",
      "3\n",
      "train battery id: 1, (914, 4, 128) (914, 1)\n",
      "3\n",
      "train battery id: 2, (678, 4, 128) (678, 1)\n",
      "3\n",
      "train battery id: 3, (989, 4, 128) (989, 1)\n",
      "3\n",
      "train battery id: 4, (1089, 4, 128) (1089, 1)\n",
      "3\n",
      "train data shape:  (3670, 4, 128) (3670, 1)\n",
      "----------------  finished !  --------------------\n",
      "----------- load features -------------\n",
      "2\n",
      "test battery id: 2, test data shape: (678, 67), (678, 1)\n",
      "2\n",
      "train battery id: 1, (914, 67), (914, 1)\n",
      "2\n",
      "train battery id: 3, (989, 67), (989, 1)\n",
      "2\n",
      "train battery id: 4, (1089, 67), (1089, 1)\n",
      "2\n",
      "train battery id: 5, (751, 67), (751, 1)\n",
      "train data shape:  (3743, 67) (3743, 1)\n",
      "---------------  finished !  ----------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    def get_args():\n",
    "\n",
    "        parser = argparse.ArgumentParser(description='dataloader test')\n",
    "        parser.add_argument('--random_seed',type=int,default=2023)\n",
    "        # data\n",
    "        parser.add_argument('--data', type=str, default='MIT', choices=['XJTU', 'MIT'])\n",
    "        parser.add_argument('--input_type', type=str, default='charge', choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "        parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "        parser.add_argument('--minmax_range', type=tuple, default=(-1, 1), choices=[(0, 1), (-1, 1)])\n",
    "        parser.add_argument('--batch_size', type=int, default=64)\n",
    "        # the parameters for XJTU data\n",
    "        parser.add_argument('--batch', type=int, default=2, choices=[1, 2, 3, 4, 5])\n",
    "\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        return args\n",
    "\n",
    "\n",
    "    args = get_args()\n",
    "    data = MITDdataset(args)\n",
    "    charge_data = data.get_charge_data(test_battery_id=3)\n",
    "    partial_charge = data.get_partial_data(test_battery_id=5)\n",
    "    features = data.get_features(test_battery_id=2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10ce749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c69f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResBlock(nn.Module):#ResBlock 是一个残差模块，通常用于深度神经网络中，通过在卷积路径之外添加“跳跃连接”或“残差连接”来帮助信号从输入直接传递到输出。这种结构可以有效地缓解梯度消失问题，提升深层网络的训练效果\n",
    "\n",
    "#     def __init__(self, input_channel, output_channel, stride):\n",
    "#         super(ResBlock, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv1d(input_channel, output_channel, kernel_size=3, stride=stride, padding=1),\n",
    "#             nn.BatchNorm1d(output_channel),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Conv1d(output_channel, output_channel, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm1d(output_channel)\n",
    "#         )\n",
    "\n",
    "#         self.skip_connection = nn.Sequential()\n",
    "#         if output_channel != input_channel:#如果输入通道和输出通道不相等，或步幅不为1，跳跃连接包含一个调整通道数的卷积层和一个批量归一化层，以使维度匹配。\n",
    "#             self.skip_connection = nn.Sequential(\n",
    "#                 nn.Conv1d(input_channel, output_channel, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm1d(output_channel)\n",
    "#             )\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):#前向传播过程\n",
    "#         out = self.conv(x)#输入 x 先通过卷积路径 self.conv。\n",
    "#         out = self.skip_connection(x) + out#输入 x 通过跳跃连接，与卷积路径的输出相加。\n",
    "#         out = self.relu(out)#相加后的结果通过 ReLU 激活函数输出，得到模块的输出。\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "#     def __init__(self):\n",
    "#         super(CNN,self).__init__()#旨在处理输入形状为 (N, 4, 128) 的数据，其中 N 是批量大小，4 是通道数，128 是每个通道的长度。\n",
    "#         self.layer1 = ResBlock(input_channel=4, output_channel=16, stride=1) # N,16,128\n",
    "#         self.layer2 = ResBlock(input_channel=16, output_channel=32, stride=2) # N,32,64\n",
    "#         self.layer3 = ResBlock(input_channel=32, output_channel=64, stride=2)  # N,64,32\n",
    "#         self.layer4 = ResBlock(input_channel=64, output_channel=96, stride=2)  # N,96,16\n",
    "#         self.layer5 = ResBlock(input_channel=96, output_channel=128, stride=2)  # N,128,8\n",
    "\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128 * 8, 128),#预测器将卷积层的输出展平，输入维度为 128 * 8 = 1024， 使用一个全连接层将其映射到大小为 128 的隐藏层\n",
    "#             nn.ReLU(),#用 ReLU 激活，最终映射到输出维度 1\n",
    "#             nn.Linear(128, 1)#输出 pred 为标量预测结果（形状为 (N, 1)），适用于回归任务。\n",
    "\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         '''\n",
    "#         :param x: shape:(N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = self.layer5(out)\n",
    "#         pred = self.predictor(out.view(out.size(0), -1))#在最后一个卷积层输出后，将数据展平，并通过 predictor 进行回归预测。\n",
    "#         return pred#输出 pred：为形状 (N, 1) 的张量，通常用作回归输出。\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)#生成了一个形状为 (30, 4, 128) 的随机张量，表示一个批量大小为30的输入数据，每个样本有4个通道、128个时间步或特征\n",
    "\n",
    "#     net = CNN()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)#输入形状：x.shape 应为 (30, 4, 128)。输出形状：根据模型设计，y.shape 应为 (30, 1)，即批量大小为30的标量预测。\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())#计算 CNN 模型的总参数量，并打印。\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0927d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.net = nn.LSTM(input_size=4,hidden_size=128,num_layers=2,batch_first=True)\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64,1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         :param x: (N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         x = x.transpose(1, 2)\n",
    "#         embed,(_,_) = self.net(x)\n",
    "#         out = embed[:,-1,:]\n",
    "#         pred = self.predictor(out)\n",
    "\n",
    "#         return pred\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)\n",
    "\n",
    "#     net = LSTM()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac418e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(128*4,256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256,128),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         :param x: (N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         x = x.view(-1,4*128)\n",
    "#         fea = self.net(x)\n",
    "#         out = self.predictor(fea)\n",
    "#         return out\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)\n",
    "\n",
    "#     net = MLP()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9212d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_dim=8, noise_level=0.01):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.noise_level = noise_level\n",
    "        self.fc1 = nn.Linear(input_size, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, input_size)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        return F.relu(self.fc1(x))\n",
    "\n",
    "    def mask(self, x):\n",
    "        return x + self.noise_level * torch.randn_like(x)\n",
    "\n",
    "    def decoder(self, x):\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, C]\n",
    "        x_noisy = self.mask(x)\n",
    "        encoded = self.encoder(x_noisy)  # [B, T, H]\n",
    "        decoded = self.decoder(encoded)  # [B, T, C]\n",
    "        return encoded, decoded\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, D]\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, feature_size=4, hidden_dim=32, seq_len=128, num_layers=1, nhead=2, dropout=0.1, noise_level=0.01):\n",
    "        super(Informer, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.seq_len = seq_len\n",
    "        self.auto_hidden = feature_size * 2  # e.g. 8 if feature_size=4\n",
    "\n",
    "        self.autoencoder = Autoencoder(input_size=feature_size, hidden_dim=self.auto_hidden, noise_level=noise_level)\n",
    "        self.pos = PositionalEncoding(seq_len, self.auto_hidden)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.auto_hidden, nhead=nhead, dim_feedforward=hidden_dim,\n",
    "            dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.linear = nn.Linear(seq_len * self.auto_hidden, 1)\n",
    "\n",
    "    def forward(self, x):  # Expected input: [B, C, T] or [B, T, C]\n",
    "        # 自动处理输入格式 [B, C, T] → [B, T, C]\n",
    "        if x.shape[1] == self.feature_size and x.shape[2] == self.seq_len:\n",
    "            x = x.transpose(1, 2)  # 转换为 [B, T, C]\n",
    "\n",
    "        batch_size, seq_len, feature_size = x.shape\n",
    "        encoded, _ = self.autoencoder(x)         # [B, T, auto_hidden]\n",
    "        encoded = self.pos(encoded)              # 添加位置编码\n",
    "        encoded = self.encoder(encoded)          # Transformer 编码\n",
    "        flattened = encoded.reshape(batch_size, -1)  # 展平为 [B, T*D]\n",
    "        output = self.linear(flattened)               # 回归预测 [B, 1]\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ec840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daa3c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed54fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e705ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe29c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf90393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca102f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import rand\n",
    "class SOHMode(nn.Module):\n",
    "    '''\n",
    "    data shape:\n",
    "    charge_data (N,4,128)\n",
    "    partial_data (N,4,128)\n",
    "    features (N,1,67)\n",
    "    '''\n",
    "    #args (Namespace): 配置参数，包括设备（device）、学习率（lr）和权重衰减（weight_decay）等。\n",
    "    def __init__(self,args):\n",
    "        super(SOHMode,self).__init__()\n",
    "        self.args = args  # 保存配置参数\n",
    "        self.pre_net = self._preprocessing_net()  # 预处理网络，用于特征提取或预处理\n",
    "        self.backbone = self._backbone()  # 主干网络，用于模型的主要预测逻辑\n",
    "        self.pre_net.to(args.device)  # 将预处理网络放到指定设备\n",
    "        self.backbone.to(args.device)  # 将主干网络放到指定设备\n",
    "        # 初始化模型权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=self.args.lr, weight_decay=self.args.weight_decay)\n",
    "\n",
    "        # 学习率调度器\n",
    "        #self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer,milestones=[30, 70], gamma=0.5 )\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=10, factor=0.5)\n",
    "       # 损失函数\n",
    "       #self.mse = torch.nn.MSELoss()\n",
    "        self.criterion = torch.nn.SmoothL1Loss()  # 替换 MSELoss 更鲁棒\n",
    "        self.lambda_mono = 0.5\n",
    "\n",
    "        # 存储损失的平均值\n",
    "        self.loss_meter = AverageMeter()\n",
    "        # 损失分项记录\n",
    "        self.loss_smoothl1_list = []\n",
    "        self.loss_monotonic_list = []\n",
    "\n",
    "        # 存储最佳状态\n",
    "        self.best_state = None\n",
    "        \n",
    "        # ======== 标签归一化参数（建议） ========\n",
    "        self.label_mean = args.label_mean if hasattr(args, 'label_mean') else 0.8\n",
    "        self.label_std = args.label_std if hasattr(args, 'label_std') else 0.1\n",
    "\n",
    "    def _preprocessing_net(self):\n",
    "        '''\n",
    "        A preprocess network which transform data from different sources into the same shape\n",
    "        :return: A network, with output shape (N,4,128)\n",
    "        '''\n",
    "        if self.args.input_type == 'charge': # charge_data (N,4,128)\n",
    "            net = nn.Conv1d(in_channels=4,out_channels=4,kernel_size=1)\n",
    "        elif self.args.input_type == 'partial_charge': #partial_data (N,4,128)\n",
    "            net = nn.Conv1d(in_channels=4, out_channels=4, kernel_size=1)\n",
    "        else:  # features (N,1,67)\n",
    "            net = nn.Linear(67,128*4) #(N,128*4)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    def _backbone(self):\n",
    "        if self.args.model == 'Informer':\n",
    "            return Informer(\n",
    "                feature_size=self.args.enc_in,  # 假设 enc_in=4\n",
    "                hidden_dim=32,\n",
    "                seq_len=128,\n",
    "                nhead=2,\n",
    "                dropout=0.1,\n",
    "                noise_level=0.01\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {self.args.model}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):#前向传播函数，根据输入类型调用预处理网络并将其输入到主干网络中。\n",
    "        if self.args.input_type == 'handcraft_features':  # 仅在特征输入时应用预处理\n",
    "            x = self.pre_net(x)  # 使用全连接层将特征转换\n",
    "            x = x.view(-1, 4, 128)  # 重塑张量形状\n",
    "        out = self.backbone(x)  # 使用主干网络进行预测\n",
    "        return out\n",
    "\n",
    "    def _train_one_epoch(self, train_loader, e):\n",
    "        # 将预处理网络和主干网络设为训练模式\n",
    "        self.pre_net.train()\n",
    "        self.backbone.train()\n",
    "\n",
    "        # 重置损失监测器\n",
    "        self.loss_meter.reset()\n",
    "\n",
    "        # 遍历训练数据\n",
    "        for data, label in train_loader:\n",
    "            # 将数据和标签加载到指定设备上\n",
    "            data, label = data.to(self.args.device), label.to(self.args.device)\n",
    "        \n",
    "            # 标签归一化（训练阶段）\n",
    "            label_norm = (label - self.label_mean) / self.label_std\n",
    "           \n",
    "            pred = self.forward(data)\n",
    "            \n",
    "            loss_main = self.criterion(pred, label_norm)\n",
    "            loss_mono = monotonic_loss(pred)  # ← 无论如何都先定义\n",
    "\n",
    "            if e <= 5:\n",
    "                loss = loss_main  # 前几轮不要加入单调性\n",
    "            else:\n",
    "                loss = loss_main + self.lambda_mono * loss_mono\n",
    "\n",
    "            # 梯度清零、反向传播并更新参数\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)  # 防止梯度爆炸\n",
    "            self.optimizer.step()\n",
    "  \n",
    "            # 更新损失监测器\n",
    "            self.loss_meter.update(loss.item())\n",
    "            self.loss_smoothl1_list.append(loss_main.item())\n",
    "            self.loss_monotonic_list.append(loss_mono.item())\n",
    "\n",
    "\n",
    "    def predict(self,test_loader):\n",
    "        # 将预处理网络和主干网络设为评估模式\n",
    "        self.pre_net.eval()\n",
    "        self.backbone.eval()\n",
    "        self.loss_meter.reset()\n",
    "        \n",
    "        # 重置损失监测器 \n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        \n",
    "        # 禁用梯度计算，以提高推理效率\n",
    "        with torch.no_grad():\n",
    "            # 遍历测试数据\n",
    "            for data, label in test_loader:\n",
    "                 # 将数据和标签加载到指定设备上\n",
    "                data, label = data.to(self.args.device), label.to(self.args.device)\n",
    "                # 前向传播，获取预测结果\n",
    "                pred = self.forward(data)\n",
    "                \n",
    "                 # 反归一化预测值\n",
    "                pred = pred * self.label_std + self.label_mean\n",
    "                \n",
    "                loss = self.criterion((pred - self.label_mean) / self.label_std, (label - self.label_mean) / self.label_std)\n",
    "\n",
    "                # 更新损失监测器\n",
    "                self.loss_meter.update(loss.item())\n",
    "                # 转换为 NumPy 并 flatten，避免嵌套列表结构\n",
    "                true_label.append(label.cpu().numpy().astype(np.float64).reshape(-1))\n",
    "                pred_label.append(pred.cpu().numpy().astype(np.float64).reshape(-1))\n",
    "\n",
    "            # 拼接为完整数组\n",
    "        self.true_label = np.concatenate(true_label)\n",
    "        self.pred_label = np.concatenate(pred_label)\n",
    "\n",
    "        return self.true_label, self.pred_label\n",
    "\n",
    "    def Train(self,train_loader,valid_loader,test_loader,save_folder=None):\n",
    "        min_loss = 10 #记录验证集的最低损失，初始值设为10。\n",
    "        stop = 0 #记录早停计数器，如果训练损失未改善超过一定轮数，则停止训练。\n",
    "        self.train_loss = []#列表分别存储每个 epoch 的训练和验证损失\n",
    "        self.valid_loss = []#用于保存测试集的真实和预测标签\n",
    "        \n",
    "        # 新增指标保存列表\n",
    "        self.valid_MAE = []\n",
    "        self.valid_MAPE = []\n",
    "        self.valid_R2 = []\n",
    "    \n",
    "        \n",
    "        self.test_loss = []\n",
    "        self.true_label, self.pred_label = None, None\n",
    "        \n",
    "        \n",
    "        self.best_MAE = float('inf')\n",
    "        self.best_MAPE = float('inf')\n",
    "        self.best_R2 = -float('inf')\n",
    "        \n",
    "        print(\"valid size:\", len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "\n",
    "        for e in range(1,self.args.n_epoch+1):\n",
    "            self._train_one_epoch(train_loader, e)\n",
    "            train_l = self.loss_meter.avg\n",
    "            self.train_loss.append(train_l)\n",
    "            \n",
    "            stop += 1\n",
    "            #计算验证集的预测和损失\n",
    "            self.predict(valid_loader)\n",
    "            valid_l = self.loss_meter.avg\n",
    "            self.valid_loss.append(valid_l)\n",
    "            self.scheduler.step(valid_l)#更新学习率\n",
    "            # 计算额外指标\n",
    "            errors = eval_metrix(self.true_label, self.pred_label)\n",
    "            self.valid_MAE.append(errors['MAE'])\n",
    "            self.valid_MAPE.append(errors['MAPE'])\n",
    "            self.valid_R2.append(errors['R2'])\n",
    "            \n",
    "            #从优化器中提取学习率并打印训练信息（当前 epoch、训练损失、验证损失和学习率）\n",
    "            lr = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            \n",
    "            print(f\"epoch=[{e}/{self.args.n_epoch}]  train loss : {train_l:.7f}  valid loss : {valid_l:.7f}  lr : {lr:.7f}\")\n",
    "            \n",
    "            if e % 10 == 0:\n",
    "                print(\"\")\n",
    "\n",
    "            #如果当前验证损失 valid_l 低于 min_loss，保存当前模型状态（pre_net 和 backbone）并计算测试集的预测损失。\n",
    "            if valid_l < min_loss:\n",
    "                self.best_state = {'pre_net': self.pre_net.state_dict(),\n",
    "                                   'backbone': self.backbone.state_dict()}\n",
    "                self.true_label, self.pred_label = self.predict(test_loader)\n",
    "                errors = eval_metrix(self.true_label, self.pred_label)\n",
    "\n",
    "                # 保存最佳指标（★）\n",
    "                if errors['MAE'] < self.best_MAE:\n",
    "                    self.best_MAE = errors['MAE']\n",
    "                if errors['MAPE'] < self.best_MAPE:\n",
    "                    self.best_MAPE = errors['MAPE']\n",
    "                if errors['R2'] > self.best_R2:\n",
    "                    self.best_R2 = errors['R2']\n",
    "\n",
    "\n",
    "                print(f' ------ test loss : {self.loss_meter.avg:.7f}')\n",
    "                for k, v in errors.items():\n",
    "                    print(f\"{k} : {v:.7f}\")\n",
    "\n",
    "                min_loss = valid_l\n",
    "                stop = 0\n",
    "           \n",
    "        if save_folder is not None:\n",
    "            self.save_all(save_folder)#调用 save_all(save_folder) 保存模型和训练结果。\n",
    "             # Plot the loss after training\n",
    "            self._plot_loss(self.train_loss, self.valid_loss)\n",
    "            true_label, pred_label = self.predict(test_loader)\n",
    "            \n",
    "            self._plot_metrics(self.valid_MAE, self.valid_MAPE, self.valid_R2)\n",
    "            self.plot_soh_predictions(self.true_label, self.pred_label)\n",
    "            self.plot_soh_predictions(self.true_label, self.pred_label)\n",
    "            \n",
    "\n",
    "\n",
    "    def save_all(self,folder):#保存模型的训练结果和最佳模型的状态\n",
    "        if not os.path.exists(folder):#如果指定的 folder 路径不存在，则创建该文件夹以存储模型数据和结果。\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        prefix = self.args.model + '_' + self.args.input_type#以模型名称和输入类型为前缀，方便文件区分\n",
    "        errors = eval_metrix(self.true_label,self.pred_label)#利用 eval_metrix 函数将真实标签和预测标签进行比较得到误差\n",
    "        np.savez(os.path.join(folder, f'{prefix}_results.npz'),\n",
    "            train_loss=np.array(self.train_loss),\n",
    "            valid_loss=np.array(self.valid_loss),\n",
    "            valid_MAE=np.array(self.valid_MAE),\n",
    "            valid_MAPE=np.array(self.valid_MAPE),\n",
    "            valid_R2=np.array(self.valid_R2),\n",
    "            true_label=np.array(self.true_label),\n",
    "            pred_label=np.array(self.pred_label),\n",
    "            test_errors=np.array(errors),\n",
    "            best_MAE=np.array(self.best_MAE),\n",
    "            best_MAPE=np.array(self.best_MAPE),\n",
    "            best_R2=np.array(self.best_R2),\n",
    "\n",
    "            loss_smoothl1=np.array(self.loss_smoothl1_list),\n",
    "            loss_monotonic=np.array(self.loss_monotonic_list),\n",
    "            lambda_mono=np.array(self.lambda_mono)\n",
    "        )\n",
    "        torch.save(self.best_state, os.path.join(folder, f'{prefix}_model.pkl'))\n",
    "\n",
    "    def _plot_loss(self,train_loss,valid_loss):#绘制训练和验证集的损失变化趋势\n",
    "\n",
    "        self.fig_loss = plt.figure()\n",
    "        plt.plot(train_loss,label='train')\n",
    "        plt.plot(valid_loss,label='valid')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()#显示训练损失（train）和验证损失（valid）的标签，方便辨识\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def _plot_metrics(self, MAE_list, MAPE_list, R2_list):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(MAE_list, label='MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(MAPE_list, label='MAPE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('Validation MAPE')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(R2_list, label='R2 Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('R2')\n",
    "        plt.title('Validation R2 Score')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def plot_soh_predictions(self,true_label, pred_label):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(true_label, label='True SOH', color='blue', linewidth=2)\n",
    "        plt.plot(pred_label, label='Predicted SOH', color='orange', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.title('True vs Predicted SOH Values')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('SOH Value')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def _initialize_weights(self):#用于为模型的各层权重进行初始化。权重初始化是深度学习模型训练中非常重要的一步，能够帮助模型更好、更快地收敛。\n",
    "        for m in self.modules():#通过 self.modules() 遍历模型的所有子模块（层），根据每个子模块的类型来选择合适的初始化方式。\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')#使用 kaiming_normal_ 进行权重初始化，这是一种适合ReLU激活函数的初始化方法，有助于防止梯度消失或爆炸。\n",
    "                if m.bias is not None:#如果卷积层有偏置（bias），则将其初始化为 0。\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)#将批归一化层的权重 weight 初始化为1，保持其默认缩放。\n",
    "                nn.init.constant_(m.bias, 0)#将偏置 bias 初始化为0，确保批归一化层的均值偏置初始为零偏移。\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)#使用 nn.init.normal_ 方法将线性层的权重 weight 初始化为均值为0、标准差为0.01的正态分布。\n",
    "                nn.init.constant_(m.bias, 0)#将偏置 bias 初始化为0，以确保线性层的初始输出没有偏移。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b663eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Informer(\n",
      "  (autoencoder): Autoencoder(\n",
      "    (fc1): Linear(in_features=4, out_features=8, bias=True)\n",
      "    (fc2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  )\n",
      "  (pos): PositionalEncoding()\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
      "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (linear): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n",
      "Input shape: torch.Size([30, 128, 4])\n",
      "Output shape: torch.Size([30, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def get_args():\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description='A benchmark for SOH estimation')\n",
    "        parser.add_argument('--random_seed', type=int, default=2023)\n",
    "        # data\n",
    "        parser.add_argument('--data', type=str, default='MIT', choices=['XJTU', 'MIT'])\n",
    "        parser.add_argument('--input_type', type=str, default='charge',\n",
    "                            choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "        parser.add_argument('--batch_size', type=int, default=64)\n",
    "        parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "        parser.add_argument('--minmax_range', type=tuple, default=(-1, 1), choices=[(0, 1), (-1, 1)])\n",
    "        parser.add_argument('--batch', type=int, default=9, choices=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "        # model\n",
    "        parser.add_argument('--model', type=str, default='Informer', choices=['Informer', 'TransCNN','CNN', 'LSTM', 'GRU', 'MLP', 'Attention'])\n",
    "\n",
    "        parser.add_argument('--lr', type=float, default=2e-3)\n",
    "        parser.add_argument('--weight_decay', default=5e-4)\n",
    "        parser.add_argument('--n_epoch', type=int, default=150)\n",
    "        parser.add_argument('--early_stop', default=20)\n",
    "        parser.add_argument('--device', default='cpu')\n",
    "        \n",
    "        # Informer 专用参数（你当前的报错来自它）\n",
    "        parser.add_argument('--enc_in', type=int, default=4)       # 输入维度\n",
    "        parser.add_argument('--dec_in', type=int, default=4)\n",
    "        parser.add_argument('--c_out', type=int, default=1)\n",
    "        parser.add_argument('--seq_len', type=int, default=128)    # 输入序列长度\n",
    "        parser.add_argument('--label_len', type=int, default=64)   # 编码长度\n",
    "        parser.add_argument('--out_len', type=int, default=1)      # 输出序列长度\n",
    "\n",
    "        # Use parse_known_args to ignore unrecognized arguments\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        args.label_mean = 0.8\n",
    "        args.label_std = 0.1\n",
    "        return args\n",
    "\n",
    "\n",
    "    args = get_args()\n",
    "    model = SOHMode(args)\n",
    "    #用于模型的随机输入张量，模拟实际的电池数据输入\n",
    "    x1 = torch.rand(30,4,128).to('cpu') #形状为 (30, 4, 128) 的张量，表示批次大小为 30，每个样本有 4 个特征通道，特征长度为 128。\n",
    "    x2 = torch.rand(30,1,67) #形状为 (30, 1, 67) 的张量，用于可能的附加特征。\n",
    "    enc_in = 4      # 输入通道数\n",
    "    d_model = 64    # Transformer特征维度\n",
    "    out_len = 1     # 预测输出长度\n",
    "\n",
    "    model = Informer(\n",
    "                feature_size=4,  # 假设 enc_in=4\n",
    "                hidden_dim=32,\n",
    "                seq_len=128,\n",
    "                nhead=2,\n",
    "                dropout=0.1,\n",
    "                noise_level=0.01)\n",
    "    print(model)\n",
    "\n",
    "    x = torch.rand(30, 128, enc_in)  # batch=30, seq_len=128, channels=4\n",
    "    print(\"Input shape:\", x.shape)\n",
    "\n",
    "    y = model(x)\n",
    "    print(\"Output shape:\", y.shape)  # 预期 torch.Size([30, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91c36f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CrossVal] Batch=9, Fold=4, Model=Informer, Input=charge, Norm=minmax, Exp=1\n",
      "----------- load charge data -------------\n",
      "test battery id: 4, test data shape: (506, 4, 128) (506, 1)\n",
      "3\n",
      "train battery id: 1, (443, 4, 128) (443, 1)\n",
      "3\n",
      "train battery id: 2, (454, 4, 128) (454, 1)\n",
      "3\n",
      "train battery id: 3, (477, 4, 128) (477, 1)\n",
      "3\n",
      "train battery id: 5, (600, 4, 128) (600, 1)\n",
      "3\n",
      "train data shape:  (1974, 4, 128) (1974, 1)\n",
      "-------------  finished !  ---------------\n",
      "valid size: 395\n",
      "epoch=[1/100]  train loss : 0.1401850  valid loss : 0.0493534  lr : 0.0020000\n",
      " ------ test loss : 0.0737433\n",
      "MAE : 0.0319980\n",
      "MAPE : 0.0344078\n",
      "MSE : 0.0014321\n",
      "RMSE : 0.0378430\n",
      "RE : 0.0338122\n",
      "R2 : -0.1594246\n",
      "epoch=[2/100]  train loss : 0.0635314  valid loss : 0.1205162  lr : 0.0020000\n",
      "epoch=[3/100]  train loss : 0.1135692  valid loss : 0.0650050  lr : 0.0020000\n",
      "epoch=[4/100]  train loss : 0.0605543  valid loss : 0.0426251  lr : 0.0020000\n",
      " ------ test loss : 0.0466060\n",
      "MAE : 0.0199283\n",
      "MAPE : 0.0219361\n",
      "MSE : 0.0008850\n",
      "RMSE : 0.0297493\n",
      "RE : 0.0210582\n",
      "R2 : 0.2834892\n",
      "epoch=[5/100]  train loss : 0.0750932  valid loss : 0.0379782  lr : 0.0020000\n",
      " ------ test loss : 0.0369981\n",
      "MAE : 0.0192685\n",
      "MAPE : 0.0210113\n",
      "MSE : 0.0007029\n",
      "RMSE : 0.0265124\n",
      "RE : 0.0203609\n",
      "R2 : 0.4309277\n",
      "epoch=[6/100]  train loss : 0.0903316  valid loss : 0.0593634  lr : 0.0020000\n",
      "epoch=[7/100]  train loss : 0.0339082  valid loss : 0.0190991  lr : 0.0020000\n",
      " ------ test loss : 0.0193611\n",
      "MAE : 0.0100327\n",
      "MAPE : 0.0112547\n",
      "MSE : 0.0003611\n",
      "RMSE : 0.0190028\n",
      "RE : 0.0106016\n",
      "R2 : 0.7076490\n",
      "epoch=[8/100]  train loss : 0.0212771  valid loss : 0.0141488  lr : 0.0020000\n",
      " ------ test loss : 0.0152540\n",
      "MAE : 0.0121523\n",
      "MAPE : 0.0132077\n",
      "MSE : 0.0002903\n",
      "RMSE : 0.0170368\n",
      "RE : 0.0128413\n",
      "R2 : 0.7650103\n",
      "epoch=[9/100]  train loss : 0.0349884  valid loss : 0.0229228  lr : 0.0020000\n",
      "epoch=[10/100]  train loss : 0.0414796  valid loss : 0.0120945  lr : 0.0020000\n",
      "\n",
      " ------ test loss : 0.0112209\n",
      "MAE : 0.0098172\n",
      "MAPE : 0.0106787\n",
      "MSE : 0.0002126\n",
      "RMSE : 0.0145813\n",
      "RE : 0.0103738\n",
      "R2 : 0.8278678\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 172\u001b[0m\n\u001b[0;32m    169\u001b[0m     multi_task_MIT()\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossval_mit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 172\u001b[0m     \u001b[43mcrossval_mit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInformer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcharge\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalization_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mminmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 143\u001b[0m, in \u001b[0;36mcrossval_mit\u001b[1;34m(models, input_types, normalization_types, test_ids, batches, experiment_num)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[CrossVal] Batch=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Fold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Norm=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Exp=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/crossval/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnorm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_copy\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/batch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-fold\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/experiment\u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 143\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs_copy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 57\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(args, save_prefix)\u001b[0m\n\u001b[0;32m     55\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m load_data(args, test_battery_id\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mtest_battery_id)\n\u001b[0;32m     56\u001b[0m model \u001b[38;5;241m=\u001b[39m SOHMode(args)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model, data_loader\n\u001b[0;32m     60\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[14], line 181\u001b[0m, in \u001b[0;36mSOHMode.Train\u001b[1;34m(self, train_loader, valid_loader, test_loader, save_folder)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(valid_loader\u001b[38;5;241m.\u001b[39mdataset))\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m     train_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_meter\u001b[38;5;241m.\u001b[39mavg\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loss\u001b[38;5;241m.\u001b[39mappend(train_l)\n",
      "Cell \u001b[1;32mIn[14], line 110\u001b[0m, in \u001b[0;36mSOHMode._train_one_epoch\u001b[1;34m(self, train_loader, e)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# 梯度清零、反向传播并更新参数\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 110\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)  \u001b[38;5;66;03m# 防止梯度爆炸\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='A benchmark for SOH estimation')\n",
    "    parser.add_argument('--random_seed', type=int, default=2023)\n",
    "    # data\n",
    "    parser.add_argument('--data', type=str, default='MIT',choices=['XJTU','MIT'])\n",
    "    parser.add_argument('--input_type',type=str,default='charge',choices=['charge','partial_charge','handcraft_features'])\n",
    "    parser.add_argument('--test_battery_id',type=int,default=1,help='test battery id, 1-8 for XJTU (1-15 for batch-2), 1-5 for MIT')\n",
    "    parser.add_argument('--batch_size',type=int,default=64)\n",
    "    parser.add_argument('--normalized_type',type=str,default='minmax',choices=['minmax','standard'])\n",
    "    parser.add_argument('--minmax_range',type=tuple,default=(-1,1),choices=[(0,1),(-1,1)])\n",
    "    parser.add_argument('--batch', type=int, default=9,choices=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "    # model\n",
    "    parser.add_argument('--model',type=str,default='Informer', choices=['Informer', 'TransCNN','CNN','LSTM','GRU','MLP','Attention'])\n",
    "    # CNN lr=2e-3  early_stop=30\n",
    "\n",
    "    parser.add_argument('--lr',type=float,default=2e-3)\n",
    "    parser.add_argument('--weight_decay', default=5e-4)\n",
    "    parser.add_argument('--n_epoch',type=int,default=100)\n",
    "    parser.add_argument('--early_stop',default=30)\n",
    "    parser.add_argument('--device',default='cpu')\n",
    "    parser.add_argument('--save_folder',default='results')\n",
    "    parser.add_argument('--experiment_num',default=1,type=int,help='The number of times you want to repeat the same experiment')\n",
    "    parser.add_argument('--test_ids', type=int, nargs='+', default=[1], help='List of test battery IDs to evaluate')\n",
    "    \n",
    "    # Informer 专用参数（你当前的报错来自它）\n",
    "    parser.add_argument('--enc_in', type=int, default=4)       # 输入维度\n",
    "    parser.add_argument('--dec_in', type=int, default=4)\n",
    "    parser.add_argument('--c_out', type=int, default=1)\n",
    "    parser.add_argument('--seq_len', type=int, default=128)    # 输入序列长度\n",
    "    parser.add_argument('--label_len', type=int, default=64)   # 编码长度\n",
    "    parser.add_argument('--out_len', type=int, default=1)      # 输出序列长度\n",
    "\n",
    "    # Use parse_known_args to ignore unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def load_data(args, test_battery_id):\n",
    "    loader = XJTUDdataset(args) if args.data == 'XJTU' else MITDdataset(args)\n",
    "    if args.input_type == 'charge':\n",
    "        return loader.get_charge_data(test_battery_id)\n",
    "    elif args.input_type == 'partial_charge':\n",
    "        return loader.get_partial_data(test_battery_id)\n",
    "    else:\n",
    "        return loader.get_features(test_battery_id)\n",
    "\n",
    "def run_experiment(args, save_prefix):\n",
    "    try:\n",
    "        data_loader = load_data(args, test_battery_id=args.test_battery_id)\n",
    "        model = SOHMode(args)\n",
    "        model.Train(data_loader['train'], data_loader['valid'], data_loader['test'],\n",
    "                    save_folder=save_prefix)\n",
    "        del model, data_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as ex:\n",
    "        print(f\"[Error] Batch {args.batch}, TestBattery {args.test_battery_id}, Model {args.model}, Input {args.input_type}: {ex}\")\n",
    "        traceback.print_exc()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    \n",
    "def main(args, batches=None, test_ids=None):\n",
    "    batches = batches or [args.batch]\n",
    "    test_ids = test_ids or [args.test_battery_id]\n",
    "\n",
    "    for batch in batches:\n",
    "        for test_id in test_ids:\n",
    "            args.batch = batch\n",
    "            args.test_battery_id = test_id\n",
    "            for e in range(args.experiment_num):\n",
    "                print(f\"\\nRunning: {args.model}, Batch={batch}, TestID={test_id}, Exp={e + 1}\")\n",
    "                save_path = f'results/{args.data}-{args.input_type}/{args.model}/batch{batch}-testbattery{test_id}/experiment{e + 1}'\n",
    "                run_experiment(args, save_path)\n",
    "\n",
    "\n",
    "def multi_task_XJTU():  # 一次性训练所有模型和所有输入类型\n",
    "    args = get_args()\n",
    "    setattr(args,'data','XJTU')\n",
    "    for m in ['Informer', 'TransCNN','CNN','MLP','Attention','LSTM','GRU']:\n",
    "        for type in ['handcraft_features','charge','partial_charge']:\n",
    "            setattr(args, 'model', m)\n",
    "            setattr(args, 'input_type',type)\n",
    "            main(args)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def multi_task_MIT():\n",
    "    args = get_args()\n",
    "    for norm in ['standard', 'minmax']:\n",
    "        args.normalized_type = norm\n",
    "        args.minmax_range = (0, 1)\n",
    "        args.data = 'MIT'\n",
    "        for m in ['GRU']:\n",
    "            for input_type in ['partial_charge']:\n",
    "                args.model = m\n",
    "                args.input_type = input_type\n",
    "                for batch in range(1, 10):\n",
    "                    args.batch = batch\n",
    "                    for test_id in [1, 2, 3, 4, 5]:\n",
    "                        args.test_battery_id = test_id\n",
    "                        for e in range(5):\n",
    "                            print(f\"\\n[MIT Run] Model={args.model}, Batch={batch}, TestID={test_id}, Input={input_type}, Norm={norm}, Exp={e + 1}\")\n",
    "                            save_path = f'results/{norm}/{args.data}-{args.input_type}/{args.model}/batch{batch}-testbattery{test_id}/experiment{e + 1}'\n",
    "                            run_experiment(args, save_path)\n",
    "\n",
    "\n",
    "def crossval_mit(\n",
    "    models=['Informer'], \n",
    "    input_types=['charge', 'partial_charge', 'handcraft_features'],\n",
    "    normalization_types=['minmax', 'standard'],\n",
    "    test_ids=[1, 2, 3, 4, 5],\n",
    "    batches=range(1, 10),\n",
    "    experiment_num=1\n",
    "):\n",
    "    args = get_args()\n",
    "    args.data = 'MIT'\n",
    "    args.experiment_num = experiment_num\n",
    "\n",
    "    for norm in normalization_types:\n",
    "        args.normalized_type = norm\n",
    "        args.minmax_range = (-1, 1) if norm == 'minmax' else (0, 1)\n",
    "\n",
    "        for batch in batches:\n",
    "            for test_id in test_ids:\n",
    "                for model_name in models:\n",
    "                    for input_type in input_types:\n",
    "                        for e in range(experiment_num):\n",
    "                            args_copy = deepcopy(args)\n",
    "                            args_copy.batch = batch\n",
    "                            args_copy.test_battery_id = test_id\n",
    "                            args_copy.model = model_name\n",
    "                            args_copy.input_type = input_type\n",
    "\n",
    "                            print(f\"\\n[CrossVal] Batch={batch}, Fold={test_id}, Model={model_name}, \"\n",
    "                                  f\"Input={input_type}, Norm={norm}, Exp={e+1}\")\n",
    "                            \n",
    "                            save_path = f'results/crossval/{norm}/{args_copy.data}-{input_type}/{model_name}/batch{batch}-fold{test_id}/experiment{e+1}'\n",
    "                            run_experiment(args_copy, save_path)\n",
    "\n",
    "                          \n",
    "                            \n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "#     from argparse import ArgumentParse\n",
    "\n",
    "#     parser = ArgumentParser()\n",
    "#     parser.add_argument('--task', type=str, default='crossval_mit', choices=[\n",
    "#         'single', 'multi_xjtu', 'multi_mit', 'crossval_mit'\n",
    "#     ])\n",
    "#     task_args, _ = parser.parse_known_args()\n",
    "    task = 'crossval_mit'\n",
    "\n",
    "    if task == 'single':\n",
    "        for e in range(args.experiment_num):\n",
    "            print(f\"\\n[Single Run] {args.model} on Battery {args.test_battery_id}, Batch {args.batch}, Input={args.input_type}\")\n",
    "            save_path = f'results/{args.data}-{args.input_type}/{args.model}/batch{args.batch}-testbattery{args.test_battery_id}/experiment{e + 1}'\n",
    "            run_experiment(args, save_path)\n",
    "\n",
    "    elif task == 'multi_xjtu':\n",
    "        multi_task_XJTU()\n",
    "\n",
    "    elif task == 'multi_mit':\n",
    "        multi_task_MIT()\n",
    "\n",
    "    elif task == 'crossval_mit':\n",
    "        crossval_mit(\n",
    "        models=['Informer'],\n",
    "        input_types=['charge'],\n",
    "        normalization_types=['minmax'],\n",
    "        test_ids=[4],\n",
    "        batches=[9],\n",
    "        experiment_num=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672569d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
