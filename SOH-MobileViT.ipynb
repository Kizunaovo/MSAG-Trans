{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49baab17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdc7d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score  # ✅ 添加这行\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaa4d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2c7555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler():#对+数据进行标准化和缩放\n",
    "    def __init__(self,data):  # data.shape (N,C,L)  or (N,C)\n",
    "        self.data = data\n",
    "        if self.data.ndim == 3: # (N,C,L)其中 N 是样本数，C 是特征数，L 表示数据的长度（如时间步长）。\n",
    "            self.mean = self.data.mean(axis=(0,2), keepdims=True)#沿 axis=(0,2)（即样本维度和长度维度）计算均值、方差、最大值和最小值\n",
    "            self.var = self.data.var(axis=(0,2), keepdims=True)#reshape(1, -1, 1) 将计算结果调整为合适的形状\n",
    "            self.max = self.data.max(axis=(0,2), keepdims=True)\n",
    "            self.min = self.data.min(axis=(0,2), keepdims=True)\n",
    "        elif self.data.ndim == 2: # (N,C)\n",
    "            self.mean = self.data.mean(axis=0, keepdims=True)\n",
    "            self.var = self.data.var(axis=0, keepdims=True)#沿 axis=0（样本维度）\n",
    "            self.max = self.data.max(axis=0, keepdims=True)\n",
    "            self.min = self.data.min(axis=0, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError('data dim error!')\n",
    "        print(self.data.ndim)\n",
    "\n",
    "    def standerd(self):#标准化方法\n",
    "        X = (self.data - self.mean) / (self.var + 1e-6)\n",
    "        return X\n",
    "\n",
    "    def minmax(self,feature_range=(0,1)):#归一化方法\n",
    "        if feature_range == (0,1):\n",
    "            X = (self.data - self.min) / ((self.max - self.min) + 1e-6)\n",
    "        elif feature_range == (-1,1):\n",
    "            X = 2*(self.data - self.min) / ((self.max - self.min) + 1e-6)-1\n",
    "        else:\n",
    "            raise ValueError('feature_range error!')\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dea6f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):#用于跟踪指标的实用工具，常用于深度学习模型的训练或评估过程中，帮助计算和维护例如损失或精度的平均值。\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):# 重置所有变量为初始状态\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):# 更新当前值、总和和计数器，计算新的平均值\n",
    "        self.val = val\n",
    "        self.sum += val * n \n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "def eval_metrix(true_label, pred_label):\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # 强制转换为 float64 类型\n",
    "    true_label = np.array(true_label, dtype=np.float64).flatten()\n",
    "    pred_label = np.array(pred_label, dtype=np.float64).flatten()\n",
    "\n",
    "    # 异常值检查\n",
    "    if np.any(np.isnan(true_label)) or np.any(np.isnan(pred_label)):\n",
    "        raise ValueError(\"true_label or pred_label contains NaN.\")\n",
    "    if np.any(np.isinf(true_label)) or np.any(np.isinf(pred_label)):\n",
    "        raise ValueError(\"true_label or pred_label contains Inf.\")\n",
    "    if np.any(np.abs(true_label) > 1e6) or np.any(np.abs(pred_label) > 1e6):\n",
    "        raise ValueError(\"true_label or pred_label contains excessively large values.\")\n",
    "\n",
    "    # 计算指标\n",
    "    MAE = metrics.mean_absolute_error(true_label, pred_label)\n",
    "    MAPE = metrics.mean_absolute_percentage_error(true_label, pred_label)\n",
    "    MSE = metrics.mean_squared_error(true_label, pred_label)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    RE = np.sum(np.abs(true_label - pred_label)) / np.sum(np.abs(true_label) + epsilon)\n",
    "    R2 = r2_score(true_label, pred_label)\n",
    "\n",
    "    return {'MAE': MAE, 'MAPE': MAPE, 'MSE': MSE, 'RMSE': RMSE, 'RE': RE, 'R2': R2}\n",
    "\n",
    "\n",
    "        \n",
    "def monotonic_loss(pred):\n",
    "    \"\"\"\n",
    "    单调性惩罚损失：只惩罚预测值的上升趋势，鼓励 SOH 下降或平稳。\n",
    "    输入 pred: Tensor, shape (B, T) 或 (B, 1)，假设时间在 dim=1 上。\n",
    "    \"\"\"\n",
    "    if pred.dim() == 2 and pred.size(1) > 1:\n",
    "        diff = pred[:, 1:] - pred[:, :-1]\n",
    "        mono_penalty = torch.relu(diff)  # 上升部分的惩罚\n",
    "        return mono_penalty.mean()\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
    "\n",
    "def save_to_txt(save_name,string):\n",
    "    f = open(save_name,mode='a')\n",
    "    f.write(string)\n",
    "    f.write('\\n')\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7453c44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class XJTUDdataset():\n",
    "#     def __init__(self,args):\n",
    "#         super().__init__()\n",
    "#         self.root = 'C:/Users/86152/PycharmProjects/SOHbenchmark-main/data/XJTU'\n",
    "#         self.max_capacity = 2.0\n",
    "#         self.normalized_type = args.normalized_type\n",
    "#         self.minmax_range = args.minmax_range\n",
    "#         self.seed = args.random_seed\n",
    "#         self.batch = args.batch\n",
    "#         self.batch_size = args.batch_size\n",
    "\n",
    "\n",
    "#     def _parser_mat_data(self,battery_i_mat):# 用于解析电池数据并返回特征和标签，标签为电池的健康状态（SOH）\n",
    "#         '''\n",
    "#         :param battery_i_mat: shape:(1,len)#特定电池的原始数据矩阵（形状为 (1, len)）\n",
    "#         :return: np.array\n",
    "#         '''\n",
    "#         data = []#存放处理后的特征数据和容量标签\n",
    "#         label = []\n",
    "#         for i in range(battery_i_mat.shape[1]):#（循环次数等于数据的第二维长度）\n",
    "#             cycle_i_data = battery_i_mat[0,i]\n",
    "#             time = cycle_i_data['relative_time_min'] # (1,128)\n",
    "#             current = cycle_i_data['current_A'] # (1,128)\n",
    "#             voltage = cycle_i_data['voltage_V'] # (1,128)\n",
    "#             temperature = cycle_i_data['temperature_C'] # (1,128)\n",
    "#             capacity = cycle_i_data['capacity'][0]\n",
    "#             label.append(capacity)\n",
    "#             cycle_i = np.concatenate([time,current,voltage,temperature],axis=0)#形状为 (4, 128)\n",
    "#             data.append(cycle_i)\n",
    "#         data = np.array(data,dtype=np.float32)\n",
    "#         label = np.array(label,dtype=np.float32)\n",
    "#         print(data.shape,label.shape)\n",
    "\n",
    "#         scaler = Scaler(data)\n",
    "#         if self.normalized_type == 'standard':\n",
    "#             data = scaler.standerd()\n",
    "#         else:\n",
    "#             data = scaler.minmax(feature_range=self.minmax_range)\n",
    "#         soh = label / self.max_capacity#容量标签 label 除以电池最大容量 self.max_capacity 进行归一化。\n",
    "\n",
    "#         return data,soh\n",
    "\n",
    "#     def _encapsulation(self,train_x,train_y,test_x,test_y):\n",
    "#         '''\n",
    "#         Encapsulate the numpy.array into DataLoader\n",
    "#         :param train_x: numpy.array\n",
    "#         :param train_y: numpy.array\n",
    "#         :param test_x: numpy.array\n",
    "#         :param test_y: numpy.array\n",
    "#         :return:\n",
    "#         '''\n",
    "#         train_x = torch.from_numpy(train_x)#转换成 PyTorch 的 DataLoader 格式，方便后续的模型训练、验证和测试\n",
    "#         train_y = torch.from_numpy(train_y)\n",
    "#         test_x = torch.from_numpy(test_x)\n",
    "#         test_y = torch.from_numpy(test_y)\n",
    "\n",
    "#         train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=self.seed)# 将训练数据 train_x 和 train_y 按 80% 的比例划分为训练集和验证集，验证集的比例为 20%。random_state=self.seed 确保分割的随机性可复现\n",
    "#         train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=self.batch_size, shuffle=True,\n",
    "#                                   drop_last=False)#shuffle=True 表示打乱数据，drop_last=False 表示保留最后一个批次，即使样本数量不足一个完整批次。\n",
    "#         valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=self.batch_size, shuffle=True,\n",
    "#                                   drop_last=False)\n",
    "#         test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=self.batch_size, shuffle=False)#但 shuffle=False，保持测试数据的顺序。\n",
    "#         return train_loader, valid_loader, test_loader\n",
    "\n",
    "#     def _get_raw_data(self,path,test_battery_id):#接受参数 path（文件路径）和 test_battery_id（测试用电池 ID）。使用 loadmat 函数加载 .mat 文件数据，并将其存储在 mat 中。\n",
    "#         mat = loadmat(path)\n",
    "#         battery = mat['battery']\n",
    "#         battery_ids = list(range(1, battery.shape[1] + 1))#从 1 到电池数量编号\n",
    "#         if test_battery_id not in battery_ids:\n",
    "#             raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "\n",
    "#         test_battery = battery[0, test_battery_id - 1][0]\n",
    "#         print(f'test battery id: {test_battery_id}, test data shape: ', end='')\n",
    "#         test_x, test_y = self._parser_mat_data(test_battery)\n",
    "        \n",
    "#         train_x, train_y = [], []\n",
    "#         for id in battery_ids:\n",
    "#             if id == test_battery_id:\n",
    "#                 continue\n",
    "#             print(f'train battery id: {id}, ', end='')\n",
    "#             train_battery = battery[0, id - 1][0]\n",
    "#             x, y = self._parser_mat_data(train_battery)\n",
    "#             train_x.append(x)\n",
    "#             train_y.append(y)\n",
    "#         train_x = np.concatenate(train_x, axis=0)\n",
    "#         train_y = np.concatenate(train_y, axis=0)\n",
    "#         print('train data shape: ', train_x.shape, train_y.shape)#将 train_x 和 train_y 中的列表项按行连接成完整的训练数据矩阵和标签向量，并打印它们的形状。\n",
    "\n",
    "#         return self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "\n",
    "#     def get_charge_data(self,test_battery_id=1):\n",
    "#         print('----------- load charge data -------------')\n",
    "#         file_name = f'batch-{self.batch}.mat'\n",
    "#         self.charge_path = os.path.join(self.root, 'charge', file_name)#根据类的 batch 属性构建文件名（如 batch-1.mat），并将其与根目录 self.root 以及子文件夹 charge 组合成充电数据文件的完整路径 self.charge_path\n",
    "#         train_loader, valid_loader, test_loader = self._get_raw_data(path=self.charge_path,test_battery_id=test_battery_id)\n",
    "#         data_dict = {'train':train_loader,\n",
    "#                      'test':test_loader,\n",
    "#                      'valid':valid_loader}\n",
    "#         print('-------------  finished !  ---------------')\n",
    "#         return data_dict\n",
    "\n",
    "\n",
    "#     def get_partial_data(self,test_battery_id=1):#加载特定电压范围内的部分充电数据\n",
    "#         print('----------- load partial_charge data -------------')\n",
    "#         file_name = f'batch-{self.batch}_3.7-4.1.mat'\n",
    "#         if self.batch == 6:\n",
    "#             file_name = f'batch-{self.batch}_3.9-4.19.mat'\n",
    "#         self.partial_path = os.path.join(self.root, 'partial_charge', file_name)\n",
    "#         train_loader, valid_loader, test_loader = self._get_raw_data(path=self.partial_path,\n",
    "#                                                                      test_battery_id=test_battery_id)\n",
    "#         data_dict = {'train': train_loader,\n",
    "#                      'test': test_loader,\n",
    "#                      'valid': valid_loader}\n",
    "#         print('----------------  finished !  --------------------')\n",
    "#         return data_dict\n",
    "\n",
    "#     def _parser_xlsx(self,df_i):\n",
    "#         '''\n",
    "#         features dataframe\n",
    "#         :param df_i: shape:(N,C+1)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         N = df_i.shape[0]\n",
    "#         x = np.array(df_i.iloc[:, :-1],dtype=np.float32)\n",
    "#         label = np.array(df_i['label'],dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "#         scaler = Scaler(x)\n",
    "#         if self.normalized_type == 'standard':\n",
    "#             data = scaler.standerd()\n",
    "#         else:\n",
    "#             data = scaler.minmax(feature_range=self.minmax_range)\n",
    "#         soh = label / self.max_capacity\n",
    "\n",
    "#         return data, soh\n",
    "\n",
    "#     def get_features(self,test_battery_id=1):#加载手工特征数据，并将数据组织为训练、验证和测试集。\n",
    "#         print('----------- load features -------------')\n",
    "#         file_name = f'batch-{self.batch}_features.xlsx'\n",
    "#         self.features_path = os.path.join(self.root, 'handcraft_features', file_name)\n",
    "#         df = pd.read_excel(self.features_path,sheet_name=None)\n",
    "#         sheet_names = list(df.keys())\n",
    "#         battery_ids = list(range(1, len(sheet_names)+1))\n",
    "\n",
    "#         if test_battery_id not in battery_ids:\n",
    "#             raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "#         test_battery_df = pd.read_excel(self.features_path,sheet_name=test_battery_id-1,header=0)\n",
    "#         test_x,test_y = self._parser_xlsx(test_battery_df)\n",
    "#         print(f'test battery id: {test_battery_id}, test data shape: {test_x.shape}, {test_y.shape}')\n",
    "\n",
    "#         train_x, train_y = [], []\n",
    "        \n",
    "#         for id in battery_ids:\n",
    "#             if id == test_battery_id:\n",
    "#                 continue\n",
    "#             sheet_name = sheet_names[id-1]\n",
    "#             df_i = df[sheet_name]\n",
    "#             x, y = self._parser_xlsx(df_i)\n",
    "#             print(f'train battery id: {id}, {x.shape}, {y.shape}')\n",
    "#             train_x.append(x)\n",
    "#             train_y.append(y)\n",
    "#         train_x = np.concatenate(train_x,axis=0)\n",
    "#         train_y = np.concatenate(train_y,axis=0)\n",
    "#         print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "#         train_loader, valid_loader, test_loader = self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "#         data_dict = {'train': train_loader,\n",
    "#                      'test': test_loader,\n",
    "#                      'valid': valid_loader}\n",
    "#         print('---------------  finished !  ----------------')\n",
    "#         return data_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce5a6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     import argparse\n",
    "#     def get_args():\n",
    "\n",
    "#         parser = argparse.ArgumentParser(description='dataloader test')\n",
    "#         parser.add_argument('--random_seed',type=int,default=2023)\n",
    "#         # data\n",
    "#         parser.add_argument('--data', type=str, default='XJTU', choices=['XJTU', 'MIT', 'CALCE'])\n",
    "#         parser.add_argument('--input_type', type=str, default='charge',\n",
    "#                             choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "#         parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "#         parser.add_argument('--minmax_range', type=tuple, default=(0, 1), choices=[(0, 1), (1, 1)])\n",
    "#         parser.add_argument('--batch_size', type=int, default=128)\n",
    "#         # the parameters for XJTU data\n",
    "#         parser.add_argument('--batch', type=int, default=1, choices=[1, 2, 3, 4, 5])\n",
    "\n",
    "#         # Use parse_known_args to ignore unrecognized arguments\n",
    "#         args, unknown = parser.parse_known_args()\n",
    "#         return args\n",
    "\n",
    "#     args = get_args()\n",
    "#     data = XJTUDdataset(args)\n",
    "#     charge_data = data.get_charge_data()\n",
    "#     partial_data = data.get_partial_data()\n",
    "#     features = data.get_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2e5b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MITDdataset():\n",
    "    def __init__(self,args):\n",
    "        super(MITDdataset).__init__()\n",
    "        self.root = 'D:/pythonproject/SOHbenchmark-main/data/MIT'\n",
    "        self.max_capacity = 1.1\n",
    "        self.normalized_type = args.normalized_type\n",
    "        self.minmax_range = args.minmax_range\n",
    "        self.seed = args.random_seed\n",
    "        self.batch = args.batch\n",
    "        self.batch_size = args.batch_size\n",
    "        if args.batch < 1 or args.batch > 9:\n",
    "            raise IndexError(f'\"batch\" must be in the [1, 9], but got {self.batch}. ')\n",
    "\n",
    "\n",
    "    def _parser_mat_data(self,battery_i_mat):\n",
    "        '''\n",
    "        :param battery_i_mat: shape:(1,len)\n",
    "        :return: np.array\n",
    "        '''\n",
    "        data = []\n",
    "        label = []\n",
    "        for i in range(battery_i_mat.shape[1]):\n",
    "            cycle_i_data = battery_i_mat[0,i]\n",
    "            time = cycle_i_data['relative_time_min'] # (1,128)\n",
    "            current = cycle_i_data['current_A'] # (1,128)\n",
    "            voltage = cycle_i_data['voltage_V'] # (1,128)\n",
    "            temperature = cycle_i_data['temperature_C'] # (1,128)\n",
    "            capacity = cycle_i_data['capacity'][0]\n",
    "            label.append(capacity)\n",
    "            cycle_i = np.concatenate([time,current,voltage,temperature],axis=0)\n",
    "            data.append(cycle_i)\n",
    "        data = np.array(data,dtype=np.float32)\n",
    "        label = np.array(label,dtype=np.float32)\n",
    "        print(data.shape,label.shape)\n",
    "\n",
    "        scaler = Scaler(data)\n",
    "        if self.normalized_type == 'standard':\n",
    "            data = scaler.standerd()\n",
    "        else:\n",
    "            data = scaler.minmax(feature_range=self.minmax_range)\n",
    "        soh = label / self.max_capacity\n",
    "\n",
    "        return data,soh\n",
    "\n",
    "    def _encapsulation(self,train_x,train_y,test_x,test_y):\n",
    "        '''\n",
    "        Encapsulate the numpy.array into DataLoader\n",
    "        :param train_x: numpy.array\n",
    "        :param train_y: numpy.array\n",
    "        :param test_x: numpy.array\n",
    "        :param test_y: numpy.array\n",
    "        :return:\n",
    "        '''\n",
    "        train_x = torch.from_numpy(train_x)\n",
    "        train_y = torch.from_numpy(train_y)\n",
    "        test_x = torch.from_numpy(test_x)\n",
    "        test_y = torch.from_numpy(test_y)\n",
    "\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(train_x, train_y, test_size=0.2, random_state=self.seed)\n",
    "        train_loader = DataLoader(TensorDataset(train_x, train_y), batch_size=self.batch_size, shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        valid_loader = DataLoader(TensorDataset(valid_x, valid_y), batch_size=self.batch_size, shuffle=True,\n",
    "                                  drop_last=False)\n",
    "        test_loader = DataLoader(TensorDataset(test_x, test_y), batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, valid_loader, test_loader\n",
    "\n",
    "    def _get_raw_data(self,path,test_battery_id):\n",
    "        mat = loadmat(path)\n",
    "        battery = mat['battery']\n",
    "        battery_ids = list(range(1, battery.shape[1] + 1))\n",
    "        if test_battery_id not in battery_ids:\n",
    "            raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "\n",
    "        test_battery = battery[0, test_battery_id - 1][0]\n",
    "        print(f'test battery id: {test_battery_id}, test data shape: ', end='')\n",
    "        test_x, test_y = self._parser_mat_data(test_battery)\n",
    "        train_x, train_y = [], []\n",
    "        for id in battery_ids:\n",
    "            if id == test_battery_id:\n",
    "                continue\n",
    "            print(f'train battery id: {id}, ', end='')\n",
    "            train_battery = battery[0, id - 1][0]\n",
    "            x, y = self._parser_mat_data(train_battery)\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        train_x = np.concatenate(train_x, axis=0)\n",
    "        train_y = np.concatenate(train_y, axis=0)\n",
    "        print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "        return self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "\n",
    "    def get_charge_data(self,test_battery_id=1):\n",
    "        print('----------- load charge data -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'charge'))\n",
    "        file_name = charge_files[self.batch-1]\n",
    "\n",
    "        self.charge_path = os.path.join(self.root, 'charge', file_name)\n",
    "        train_loader, valid_loader, test_loader = self._get_raw_data(path=self.charge_path,test_battery_id=test_battery_id)\n",
    "        data_dict = {'train':train_loader,\n",
    "                     'test':test_loader,\n",
    "                     'valid':valid_loader}\n",
    "        print('-------------  finished !  ---------------')\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "    def get_partial_data(self,test_battery_id=1):\n",
    "        print('----------- load partial_charge data -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'partial_charge'))\n",
    "        file_name = charge_files[self.batch - 1]\n",
    "        self.partial_path = os.path.join(self.root, 'partial_charge', file_name)\n",
    "        train_loader, valid_loader, test_loader = self._get_raw_data(path=self.partial_path,\n",
    "                                                                     test_battery_id=test_battery_id)\n",
    "        data_dict = {'train': train_loader,\n",
    "                     'test': test_loader,\n",
    "                     'valid': valid_loader}\n",
    "        print('----------------  finished !  --------------------')\n",
    "        return data_dict\n",
    "\n",
    "    def _parser_xlsx(self,df_i):\n",
    "        '''\n",
    "        features dataframe\n",
    "        :param df_i: shape:(N,C+1)\n",
    "        :return:\n",
    "        '''\n",
    "        N = df_i.shape[0]\n",
    "        x = np.array(df_i.iloc[:, :-1],dtype=np.float32)\n",
    "        label = np.array(df_i['label'],dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "        scaler = Scaler(x)\n",
    "        if self.normalized_type == 'standard':\n",
    "            data = scaler.standerd()\n",
    "        else:\n",
    "            data = scaler.minmax(feature_range=self.minmax_range)\n",
    "        soh = label / self.max_capacity\n",
    "\n",
    "        return data, soh\n",
    "\n",
    "    def get_features(self,test_battery_id=1):\n",
    "        print('----------- load features -------------')\n",
    "        charge_files = os.listdir(os.path.join(self.root, 'handcraft_features'))\n",
    "        file_name = charge_files[self.batch - 1]\n",
    "        self.features_path = os.path.join(self.root, 'handcraft_features', file_name)\n",
    "        df = pd.read_excel(self.features_path,sheet_name=None)\n",
    "        sheet_names = list(df.keys())\n",
    "        battery_ids = list(range(1, len(sheet_names)+1))\n",
    "\n",
    "        if test_battery_id not in battery_ids:\n",
    "            raise IndexError(f'\"test_battery\" must be in the {battery_ids}, but got {test_battery_id}. ')\n",
    "        test_battery_df = pd.read_excel(self.features_path,sheet_name=test_battery_id-1,header=0)\n",
    "        test_x,test_y = self._parser_xlsx(test_battery_df)\n",
    "        print(f'test battery id: {test_battery_id}, test data shape: {test_x.shape}, {test_y.shape}')\n",
    "\n",
    "        train_x, train_y = [], []\n",
    "        for id in battery_ids:\n",
    "            if id == test_battery_id:\n",
    "                continue\n",
    "            sheet_name = sheet_names[id-1]\n",
    "            df_i = df[sheet_name]\n",
    "            x, y = self._parser_xlsx(df_i)\n",
    "            print(f'train battery id: {id}, {x.shape}, {y.shape}')\n",
    "            train_x.append(x)\n",
    "            train_y.append(y)\n",
    "        train_x = np.concatenate(train_x,axis=0)\n",
    "        train_y = np.concatenate(train_y,axis=0)\n",
    "        print('train data shape: ', train_x.shape, train_y.shape)\n",
    "\n",
    "        train_loader, valid_loader, test_loader = self._encapsulation(train_x, train_y, test_x, test_y)\n",
    "        data_dict = {'train': train_loader,\n",
    "                     'test': test_loader,\n",
    "                     'valid': valid_loader}\n",
    "        print('---------------  finished !  ----------------')\n",
    "        return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d67791ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- load charge data -------------\n",
      "test battery id: 3, test data shape: (989, 4, 128) (989, 1)\n",
      "3\n",
      "train battery id: 1, (914, 4, 128) (914, 1)\n",
      "3\n",
      "train battery id: 2, (678, 4, 128) (678, 1)\n",
      "3\n",
      "train battery id: 4, (1089, 4, 128) (1089, 1)\n",
      "3\n",
      "train battery id: 5, (751, 4, 128) (751, 1)\n",
      "3\n",
      "train data shape:  (3432, 4, 128) (3432, 1)\n",
      "-------------  finished !  ---------------\n",
      "----------- load partial_charge data -------------\n",
      "test battery id: 5, test data shape: (751, 4, 128) (751, 1)\n",
      "3\n",
      "train battery id: 1, (914, 4, 128) (914, 1)\n",
      "3\n",
      "train battery id: 2, (678, 4, 128) (678, 1)\n",
      "3\n",
      "train battery id: 3, (989, 4, 128) (989, 1)\n",
      "3\n",
      "train battery id: 4, (1089, 4, 128) (1089, 1)\n",
      "3\n",
      "train data shape:  (3670, 4, 128) (3670, 1)\n",
      "----------------  finished !  --------------------\n",
      "----------- load features -------------\n",
      "2\n",
      "test battery id: 2, test data shape: (678, 67), (678, 1)\n",
      "2\n",
      "train battery id: 1, (914, 67), (914, 1)\n",
      "2\n",
      "train battery id: 3, (989, 67), (989, 1)\n",
      "2\n",
      "train battery id: 4, (1089, 67), (1089, 1)\n",
      "2\n",
      "train battery id: 5, (751, 67), (751, 1)\n",
      "train data shape:  (3743, 67) (3743, 1)\n",
      "---------------  finished !  ----------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    def get_args():\n",
    "\n",
    "        parser = argparse.ArgumentParser(description='dataloader test')\n",
    "        parser.add_argument('--random_seed',type=int,default=2023)\n",
    "        # data\n",
    "        parser.add_argument('--data', type=str, default='MIT', choices=['XJTU', 'MIT'])\n",
    "        parser.add_argument('--input_type', type=str, default='charge', choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "        parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "        parser.add_argument('--minmax_range', type=tuple, default=(-1, 1), choices=[(0, 1), (-1, 1)])\n",
    "        parser.add_argument('--batch_size', type=int, default=128)\n",
    "        # the parameters for XJTU data\n",
    "        parser.add_argument('--batch', type=int, default=2, choices=[1, 2, 3, 4, 5])\n",
    "\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        return args\n",
    "\n",
    "\n",
    "    args = get_args()\n",
    "    data = MITDdataset(args)\n",
    "    charge_data = data.get_charge_data(test_battery_id=3)\n",
    "    partial_charge = data.get_partial_data(test_battery_id=5)\n",
    "    features = data.get_features(test_battery_id=2)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10ce749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c69f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResBlock(nn.Module):#ResBlock 是一个残差模块，通常用于深度神经网络中，通过在卷积路径之外添加“跳跃连接”或“残差连接”来帮助信号从输入直接传递到输出。这种结构可以有效地缓解梯度消失问题，提升深层网络的训练效果\n",
    "\n",
    "#     def __init__(self, input_channel, output_channel, stride):\n",
    "#         super(ResBlock, self).__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv1d(input_channel, output_channel, kernel_size=3, stride=stride, padding=1),\n",
    "#             nn.BatchNorm1d(output_channel),\n",
    "#             nn.ReLU(),\n",
    "\n",
    "#             nn.Conv1d(output_channel, output_channel, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm1d(output_channel)\n",
    "#         )\n",
    "\n",
    "#         self.skip_connection = nn.Sequential()\n",
    "#         if output_channel != input_channel:#如果输入通道和输出通道不相等，或步幅不为1，跳跃连接包含一个调整通道数的卷积层和一个批量归一化层，以使维度匹配。\n",
    "#             self.skip_connection = nn.Sequential(\n",
    "#                 nn.Conv1d(input_channel, output_channel, kernel_size=1, stride=stride),\n",
    "#                 nn.BatchNorm1d(output_channel)\n",
    "#             )\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):#前向传播过程\n",
    "#         out = self.conv(x)#输入 x 先通过卷积路径 self.conv。\n",
    "#         out = self.skip_connection(x) + out#输入 x 通过跳跃连接，与卷积路径的输出相加。\n",
    "#         out = self.relu(out)#相加后的结果通过 ReLU 激活函数输出，得到模块的输出。\n",
    "#         return out\n",
    "\n",
    "\n",
    "\n",
    "# class CNN(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "#     def __init__(self):\n",
    "#         super(CNN,self).__init__()#旨在处理输入形状为 (N, 4, 128) 的数据，其中 N 是批量大小，4 是通道数，128 是每个通道的长度。\n",
    "#         self.layer1 = ResBlock(input_channel=4, output_channel=16, stride=1) # N,16,128\n",
    "#         self.layer2 = ResBlock(input_channel=16, output_channel=32, stride=2) # N,32,64\n",
    "#         self.layer3 = ResBlock(input_channel=32, output_channel=64, stride=2)  # N,64,32\n",
    "#         self.layer4 = ResBlock(input_channel=64, output_channel=96, stride=2)  # N,96,16\n",
    "#         self.layer5 = ResBlock(input_channel=96, output_channel=128, stride=2)  # N,128,8\n",
    "\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128 * 8, 128),#预测器将卷积层的输出展平，输入维度为 128 * 8 = 1024， 使用一个全连接层将其映射到大小为 128 的隐藏层\n",
    "#             nn.ReLU(),#用 ReLU 激活，最终映射到输出维度 1\n",
    "#             nn.Linear(128, 1)#输出 pred 为标量预测结果（形状为 (N, 1)），适用于回归任务。\n",
    "\n",
    "#         )\n",
    "\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         '''\n",
    "#         :param x: shape:(N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         out = self.layer1(x)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = self.layer5(out)\n",
    "#         pred = self.predictor(out.view(out.size(0), -1))#在最后一个卷积层输出后，将数据展平，并通过 predictor 进行回归预测。\n",
    "#         return pred#输出 pred：为形状 (N, 1) 的张量，通常用作回归输出。\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)#生成了一个形状为 (30, 4, 128) 的随机张量，表示一个批量大小为30的输入数据，每个样本有4个通道、128个时间步或特征\n",
    "\n",
    "#     net = CNN()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)#输入形状：x.shape 应为 (30, 4, 128)。输出形状：根据模型设计，y.shape 应为 (30, 1)，即批量大小为30的标量预测。\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())#计算 CNN 模型的总参数量，并打印。\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0927d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.net = nn.LSTM(input_size=4,hidden_size=128,num_layers=2,batch_first=True)\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128,64),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(64,1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         :param x: (N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         x = x.transpose(1, 2)\n",
    "#         embed,(_,_) = self.net(x)\n",
    "#         out = embed[:,-1,:]\n",
    "#         pred = self.predictor(out)\n",
    "\n",
    "#         return pred\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)\n",
    "\n",
    "#     net = LSTM()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7c073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac418e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     '''\n",
    "#     input shape: (N,4,128)\n",
    "#     '''\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(MLP, self).__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(128*4,256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(256,128),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.predictor = nn.Sequential(\n",
    "#             nn.Linear(128, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         '''\n",
    "#         :param x: (N,4,128)\n",
    "#         :return:\n",
    "#         '''\n",
    "#         x = x.view(-1,4*128)\n",
    "#         fea = self.net(x)\n",
    "#         out = self.predictor(fea)\n",
    "#         return out\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     x = torch.rand(30,4,128)\n",
    "\n",
    "#     net = MLP()\n",
    "#     y = net(x)\n",
    "#     print(x.shape,y.shape)\n",
    "\n",
    "#     num_params = sum(param.numel() for param in net.parameters())\n",
    "#     print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbe29c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 4\n",
    "numf = 64\n",
    "dim = 64\n",
    "emb_dropout = 0.2\n",
    "class MobileViTBlock(nn.Module):\n",
    "    def __init__(self, dim, depth, channel, patch_size, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.ph, self.pw = patch_size if isinstance(patch_size, (tuple, list)) else (patch_size, patch_size)\n",
    "        self.local_rep = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel),\n",
    "            nn.Conv2d(channel, dim, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(dim, channel, kernel_size=1),\n",
    "            nn.Conv2d(channel, channel, kernel_size=3, padding=1, groups=channel)\n",
    "        )\n",
    "    def unfolding(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        # 记录补边\n",
    "        pad_h = (self.ph - H % self.ph) % self.ph\n",
    "        pad_w = (self.pw - W % self.pw) % self.pw\n",
    "        if pad_h > 0 or pad_w > 0:\n",
    "            x = F.pad(x, (0, pad_w, 0, pad_h), mode='replicate')\n",
    "        H_pad, W_pad = x.shape[2], x.shape[3]\n",
    "\n",
    "        # 分块 (B, C, H//ph, W//pw, ph, pw)\n",
    "        x = x.unfold(2, self.ph, self.ph).unfold(3, self.pw, self.pw)  # (B, C, nH, nW, ph, pw)\n",
    "        x = x.permute(0, 2, 3, 1, 4, 5)  # (B, nH, nW, C, ph, pw)\n",
    "        x = x.reshape(B, -1, C, self.ph * self.pw)  # (B, N, C, P)\n",
    "        x = x.permute(0, 1, 3, 2).reshape(-1, self.ph * self.pw, C)  # (B*N, P, C)\n",
    "\n",
    "        return x, H_pad, W_pad\n",
    "\n",
    "\n",
    "    def folding(self, x, H, W):\n",
    "        # x: (B*N, P, C)\n",
    "        N_patch = (H // self.ph) * (W // self.pw)\n",
    "        B = x.shape[0] // N_patch\n",
    "        C = x.shape[2]\n",
    "\n",
    "        x = x.view(B, N_patch, self.ph * self.pw, C)  # (B, N, P, C)\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, C, N, P)\n",
    "\n",
    "        nH, nW = H // self.ph, W // self.pw\n",
    "        x = x.view(B, C, nH, nW, self.ph, self.pw)\n",
    "        x = x.permute(0, 1, 2, 4, 3, 5).contiguous()\n",
    "        x = x.view(B, C, H, W)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.local_rep(x)\n",
    "        z, H, W = self.unfolding(y)\n",
    "        z = self.transformer(z)\n",
    "        z = self.folding(z, H, W)\n",
    "        return self.project(z)\n",
    "    \n",
    "class MobileViT(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        # 如果你的输入为 (B, 4, 128)，我们需要把它 reshape 成 2D 图像，形式为 (B, 1, 4, 128)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.mvit1 = MobileViTBlock(dim=64, depth=2, channel=32, patch_size=(2, 2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.mvit2 = MobileViTBlock(dim=96, depth=2, channel=64, patch_size=(2, 2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 96, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(96, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # → (B, 1, 4, 128)\n",
    "        x = self.stem(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.mvit1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.mvit2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool(x).view(x.size(0), -1)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf90393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca102f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOHMode(nn.Module):\n",
    "    '''\n",
    "    data shape:\n",
    "    charge_data (N,4,128)\n",
    "    partial_data (N,4,128)\n",
    "    features (N,1,67)\n",
    "    '''\n",
    "    #args (Namespace): 配置参数，包括设备（device）、学习率（lr）和权重衰减（weight_decay）等。\n",
    "    def __init__(self,args):\n",
    "        super(SOHMode,self).__init__()\n",
    "        self.args = args  # 保存配置参数\n",
    "        self.pre_net = self._preprocessing_net()  # 预处理网络，用于特征提取或预处理\n",
    "        self.backbone = self._backbone()  # 主干网络，用于模型的主要预测逻辑\n",
    "        self.pre_net.to(args.device)  # 将预处理网络放到指定设备\n",
    "        self.backbone.to(args.device)  # 将主干网络放到指定设备\n",
    "        # 初始化模型权重\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),lr=self.args.lr, weight_decay=self.args.weight_decay)\n",
    "\n",
    "        # 学习率调度器\n",
    "        #self.scheduler = torch.optim.lr_scheduler.MultiStepLR(self.optimizer,milestones=[30, 70], gamma=0.5 )\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min', patience=10, factor=0.5)\n",
    "       # 损失函数\n",
    "       #self.mse = torch.nn.MSELoss()\n",
    "        self.criterion = torch.nn.SmoothL1Loss()  # 替换 MSELoss 更鲁棒\n",
    "        self.lambda_mono = 0.5\n",
    "\n",
    "        # 存储损失的平均值\n",
    "        self.loss_meter = AverageMeter()\n",
    "        # 损失分项记录\n",
    "        self.loss_smoothl1_list = []\n",
    "        self.loss_monotonic_list = []\n",
    "\n",
    "        # 存储最佳状态\n",
    "        self.best_state = None\n",
    "        \n",
    "        # ======== 标签归一化参数（建议） ========\n",
    "        self.label_mean = args.label_mean if hasattr(args, 'label_mean') else 0.8\n",
    "        self.label_std = args.label_std if hasattr(args, 'label_std') else 0.1\n",
    "\n",
    "    def _preprocessing_net(self):\n",
    "        '''\n",
    "        A preprocess network which transform data from different sources into the same shape\n",
    "        :return: A network, with output shape (N,4,128)\n",
    "        '''\n",
    "        if self.args.input_type == 'charge': # charge_data (N,4,128)\n",
    "            net = nn.Conv1d(in_channels=4,out_channels=4,kernel_size=1)\n",
    "        elif self.args.input_type == 'partial_charge': #partial_data (N,4,128)\n",
    "            net = nn.Conv1d(in_channels=4, out_channels=4, kernel_size=1)\n",
    "        else:  # features (N,1,67)\n",
    "            net = nn.Linear(67,128*4) #(N,128*4)\n",
    "\n",
    "        return net\n",
    "\n",
    "\n",
    "    def _backbone(self):#主干网络，用于执行核心的预测逻辑。根据配置中的模型名称实例化相应的模型。\n",
    "        backbone = eval(self.args.model)()\n",
    "        return backbone\n",
    "\n",
    "    def forward(self,x):#前向传播函数，根据输入类型调用预处理网络并将其输入到主干网络中。\n",
    "        if self.args.input_type == 'handcraft_features':  # 仅在特征输入时应用预处理\n",
    "            x = self.pre_net(x)  # 使用全连接层将特征转换\n",
    "            x = x.view(-1, 4, 128)  # 重塑张量形状\n",
    "        out = self.backbone(x)  # 使用主干网络进行预测\n",
    "        return out\n",
    "\n",
    "    def _train_one_epoch(self, train_loader, e):\n",
    "        # 将预处理网络和主干网络设为训练模式\n",
    "        self.pre_net.train()\n",
    "        self.backbone.train()\n",
    "\n",
    "        # 重置损失监测器\n",
    "        self.loss_meter.reset()\n",
    "\n",
    "        # 遍历训练数据\n",
    "        for data, label in train_loader:\n",
    "            # 将数据和标签加载到指定设备上\n",
    "            data, label = data.to(self.args.device), label.to(self.args.device)\n",
    "        \n",
    "            # 标签归一化（训练阶段）\n",
    "            label_norm = (label - self.label_mean) / self.label_std\n",
    "           \n",
    "            pred = self.forward(data)\n",
    "            \n",
    "            loss_main = self.criterion(pred, label_norm)\n",
    "            loss_mono = monotonic_loss(pred)  # ← 无论如何都先定义\n",
    "\n",
    "            if e <= 5:\n",
    "                loss = loss_main  # 前几轮不要加入单调性\n",
    "            else:\n",
    "                loss = loss_main + self.lambda_mono * loss_mono\n",
    "\n",
    "            # 梯度清零、反向传播并更新参数\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)  # 防止梯度爆炸\n",
    "            self.optimizer.step()\n",
    "  \n",
    "            # 更新损失监测器\n",
    "            self.loss_meter.update(loss.item())\n",
    "            self.loss_smoothl1_list.append(loss_main.item())\n",
    "            self.loss_monotonic_list.append(loss_mono.item())\n",
    "\n",
    "\n",
    "    def predict(self,test_loader):\n",
    "        # 将预处理网络和主干网络设为评估模式\n",
    "        self.pre_net.eval()\n",
    "        self.backbone.eval()\n",
    "        self.loss_meter.reset()\n",
    "        \n",
    "        # 重置损失监测器 \n",
    "        true_label = []\n",
    "        pred_label = []\n",
    "        \n",
    "        # 禁用梯度计算，以提高推理效率\n",
    "        with torch.no_grad():\n",
    "            # 遍历测试数据\n",
    "            for data, label in test_loader:\n",
    "                 # 将数据和标签加载到指定设备上\n",
    "                data, label = data.to(self.args.device), label.to(self.args.device)\n",
    "                # 前向传播，获取预测结果\n",
    "                pred = self.forward(data)\n",
    "                \n",
    "                 # 反归一化预测值\n",
    "                pred = pred * self.label_std + self.label_mean\n",
    "                \n",
    "                loss = self.criterion((pred - self.label_mean) / self.label_std, (label - self.label_mean) / self.label_std)\n",
    "\n",
    "                # 更新损失监测器\n",
    "                self.loss_meter.update(loss.item())\n",
    "                # 转换为 NumPy 并 flatten，避免嵌套列表结构\n",
    "                true_label.append(label.cpu().numpy().astype(np.float64).reshape(-1))\n",
    "                pred_label.append(pred.cpu().numpy().astype(np.float64).reshape(-1))\n",
    "\n",
    "            # 拼接为完整数组\n",
    "        self.true_label = np.concatenate(true_label)\n",
    "        self.pred_label = np.concatenate(pred_label)\n",
    "\n",
    "        return self.true_label, self.pred_label\n",
    "\n",
    "    def Train(self,train_loader,valid_loader,test_loader,save_folder=None):\n",
    "        min_loss = 10 #记录验证集的最低损失，初始值设为10。\n",
    "        stop = 0 #记录早停计数器，如果训练损失未改善超过一定轮数，则停止训练。\n",
    "        self.train_loss = []#列表分别存储每个 epoch 的训练和验证损失\n",
    "        self.valid_loss = []#用于保存测试集的真实和预测标签\n",
    "        \n",
    "        # 新增指标保存列表\n",
    "        self.valid_MAE = []\n",
    "        self.valid_MAPE = []\n",
    "        self.valid_R2 = []\n",
    "    \n",
    "        \n",
    "        self.test_loss = []\n",
    "        self.true_label, self.pred_label = None, None\n",
    "        \n",
    "        \n",
    "        self.best_MAE = float('inf')\n",
    "        self.best_MAPE = float('inf')\n",
    "        self.best_R2 = -float('inf')\n",
    "        \n",
    "        print(\"valid size:\", len(valid_loader.dataset))\n",
    "\n",
    "\n",
    "\n",
    "        for e in range(1,self.args.n_epoch+1):\n",
    "            self._train_one_epoch(train_loader, e)\n",
    "            train_l = self.loss_meter.avg\n",
    "            self.train_loss.append(train_l)\n",
    "            \n",
    "            stop += 1\n",
    "            #计算验证集的预测和损失\n",
    "            self.predict(valid_loader)\n",
    "            valid_l = self.loss_meter.avg\n",
    "            self.valid_loss.append(valid_l)\n",
    "            self.scheduler.step(valid_l)#更新学习率\n",
    "            # 计算额外指标\n",
    "            errors = eval_metrix(self.true_label, self.pred_label)\n",
    "            self.valid_MAE.append(errors['MAE'])\n",
    "            self.valid_MAPE.append(errors['MAPE'])\n",
    "            self.valid_R2.append(errors['R2'])\n",
    "            \n",
    "            #从优化器中提取学习率并打印训练信息（当前 epoch、训练损失、验证损失和学习率）\n",
    "            lr = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            \n",
    "            print(f\"epoch=[{e}/{self.args.n_epoch}]  train loss : {train_l:.7f}  valid loss : {valid_l:.7f}  lr : {lr:.7f}\")\n",
    "            \n",
    "            if e % 10 == 0:\n",
    "                print(\"\")\n",
    "\n",
    "            #如果当前验证损失 valid_l 低于 min_loss，保存当前模型状态（pre_net 和 backbone）并计算测试集的预测损失。\n",
    "            if valid_l < min_loss:\n",
    "                self.best_state = {'pre_net': self.pre_net.state_dict(),\n",
    "                                   'backbone': self.backbone.state_dict()}\n",
    "                self.true_label, self.pred_label = self.predict(test_loader)\n",
    "                errors = eval_metrix(self.true_label, self.pred_label)\n",
    "\n",
    "                # 保存最佳指标（★）\n",
    "                if errors['MAE'] < self.best_MAE:\n",
    "                    self.best_MAE = errors['MAE']\n",
    "                if errors['MAPE'] < self.best_MAPE:\n",
    "                    self.best_MAPE = errors['MAPE']\n",
    "                if errors['R2'] > self.best_R2:\n",
    "                    self.best_R2 = errors['R2']\n",
    "\n",
    "\n",
    "                print(f' ------ test loss : {self.loss_meter.avg:.7f}')\n",
    "                for k, v in errors.items():\n",
    "                    print(f\"{k} : {v:.7f}\")\n",
    "\n",
    "                min_loss = valid_l\n",
    "                stop = 0\n",
    "           \n",
    "        if save_folder is not None:\n",
    "            self.save_all(save_folder)#调用 save_all(save_folder) 保存模型和训练结果。\n",
    "             # Plot the loss after training\n",
    "            self._plot_loss(self.train_loss, self.valid_loss)\n",
    "            true_label, pred_label = self.predict(test_loader)\n",
    "            \n",
    "            self._plot_metrics(self.valid_MAE, self.valid_MAPE, self.valid_R2)\n",
    "            self.plot_soh_predictions(self.true_label, self.pred_label)\n",
    "            self.plot_soh_predictions(self.true_label, self.pred_label)\n",
    "            \n",
    "\n",
    "\n",
    "    def save_all(self,folder):#保存模型的训练结果和最佳模型的状态\n",
    "        if not os.path.exists(folder):#如果指定的 folder 路径不存在，则创建该文件夹以存储模型数据和结果。\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        prefix = self.args.model + '_' + self.args.input_type#以模型名称和输入类型为前缀，方便文件区分\n",
    "        errors = eval_metrix(self.true_label,self.pred_label)#利用 eval_metrix 函数将真实标签和预测标签进行比较得到误差\n",
    "        np.savez(os.path.join(folder, f'{prefix}_results.npz'),\n",
    "            train_loss=np.array(self.train_loss),\n",
    "            valid_loss=np.array(self.valid_loss),\n",
    "            valid_MAE=np.array(self.valid_MAE),\n",
    "            valid_MAPE=np.array(self.valid_MAPE),\n",
    "            valid_R2=np.array(self.valid_R2),\n",
    "            true_label=np.array(self.true_label),\n",
    "            pred_label=np.array(self.pred_label),\n",
    "            test_errors=np.array(errors),\n",
    "            best_MAE=np.array(self.best_MAE),\n",
    "            best_MAPE=np.array(self.best_MAPE),\n",
    "            best_R2=np.array(self.best_R2),\n",
    "\n",
    "            loss_smoothl1=np.array(self.loss_smoothl1_list),\n",
    "            loss_monotonic=np.array(self.loss_monotonic_list),\n",
    "            lambda_mono=np.array(self.lambda_mono)\n",
    "        )\n",
    "        torch.save(self.best_state, os.path.join(folder, f'{prefix}_model.pkl'))\n",
    "\n",
    "    def _plot_loss(self,train_loss,valid_loss):#绘制训练和验证集的损失变化趋势\n",
    "\n",
    "        self.fig_loss = plt.figure()\n",
    "        plt.plot(train_loss,label='train')\n",
    "        plt.plot(valid_loss,label='valid')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()#显示训练损失（train）和验证损失（valid）的标签，方便辨识\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    def _plot_metrics(self, MAE_list, MAPE_list, R2_list):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(MAE_list, label='MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.title('Validation MAE')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(MAPE_list, label='MAPE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MAPE (%)')\n",
    "        plt.title('Validation MAPE')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(R2_list, label='R2 Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('R2')\n",
    "        plt.title('Validation R2 Score')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "    def plot_soh_predictions(self,true_label, pred_label):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(true_label, label='True SOH', color='blue', linewidth=2)\n",
    "        plt.plot(pred_label, label='Predicted SOH', color='orange', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.title('True vs Predicted SOH Values')\n",
    "        plt.xlabel('Samples')\n",
    "        plt.ylabel('SOH Value')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def _initialize_weights(self):#用于为模型的各层权重进行初始化。权重初始化是深度学习模型训练中非常重要的一步，能够帮助模型更好、更快地收敛。\n",
    "        for m in self.modules():#通过 self.modules() 遍历模型的所有子模块（层），根据每个子模块的类型来选择合适的初始化方式。\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')#使用 kaiming_normal_ 进行权重初始化，这是一种适合ReLU激活函数的初始化方法，有助于防止梯度消失或爆炸。\n",
    "                if m.bias is not None:#如果卷积层有偏置（bias），则将其初始化为 0。\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)#将批归一化层的权重 weight 初始化为1，保持其默认缩放。\n",
    "                nn.init.constant_(m.bias, 0)#将偏置 bias 初始化为0，确保批归一化层的均值偏置初始为零偏移。\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)#使用 nn.init.normal_ 方法将线性层的权重 weight 初始化为均值为0、标准差为0.01的正态分布。\n",
    "                nn.init.constant_(m.bias, 0)#将偏置 bias 初始化为0，以确保线性层的初始输出没有偏移。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b663eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOHMode(\n",
      "  (pre_net): Conv1d(4, 4, kernel_size=(1,), stride=(1,))\n",
      "  (backbone): MobileViT(\n",
      "    (stem): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (mvit1): MobileViTBlock(\n",
      "      (local_rep): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "        (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (transformer): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
      "            (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
      "      )\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (mvit2): MobileViTBlock(\n",
      "      (local_rep): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "        (1): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (transformer): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=96, out_features=96, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=96, out_features=2048, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=2048, out_features=96, bias=True)\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (project): Sequential(\n",
      "        (0): Conv2d(96, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "      )\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): SiLU()\n",
      "    )\n",
      "    (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=96, out_features=1, bias=True)\n",
      "  )\n",
      "  (criterion): SmoothL1Loss()\n",
      ")\n",
      "output shape: torch.Size([30, 1])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    def get_args():\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description='A benchmark for SOH estimation')\n",
    "        parser.add_argument('--random_seed', type=int, default=2023)\n",
    "        # data\n",
    "        parser.add_argument('--data', type=str, default='MIT', choices=['XJTU', 'MIT'])\n",
    "        parser.add_argument('--input_type', type=str, default='charge',\n",
    "                            choices=['charge', 'partial_charge', 'handcraft_features'])\n",
    "        parser.add_argument('--batch_size', type=int, default=128)\n",
    "        parser.add_argument('--normalized_type', type=str, default='minmax', choices=['minmax', 'standard'])\n",
    "        parser.add_argument('--minmax_range', type=tuple, default=(-1, 1), choices=[(0, 1), (-1, 1)])\n",
    "        parser.add_argument('--batch', type=int, default=9, choices=[1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "        # model\n",
    "        parser.add_argument('--model', type=str, default='MobileViT', choices=['MobileViT','TransCNN','CNN', 'LSTM', 'GRU', 'MLP', 'Attention'])\n",
    "\n",
    "        parser.add_argument('--lr', type=float, default=2e-3)\n",
    "        parser.add_argument('--weight_decay', default=5e-4)\n",
    "        parser.add_argument('--n_epoch', type=int, default=150)\n",
    "        parser.add_argument('--early_stop', default=20)\n",
    "        parser.add_argument('--device', default='cpu')\n",
    "\n",
    "        # Use parse_known_args to ignore unrecognized arguments\n",
    "        args, unknown = parser.parse_known_args()\n",
    "        args.label_mean = 0.8\n",
    "        args.label_std = 0.1\n",
    "        return args\n",
    "\n",
    "\n",
    "    args = get_args()\n",
    "    model = SOHMode(args)\n",
    "    #用于模型的随机输入张量，模拟实际的电池数据输入\n",
    "    x1 = torch.rand(30,4,128).to('cpu') #形状为 (30, 4, 128) 的张量，表示批次大小为 30，每个样本有 4 个特征通道，特征长度为 128。\n",
    "    x2 = torch.rand(30,1,67) #形状为 (30, 1, 67) 的张量，用于可能的附加特征。\n",
    "\n",
    "    y = model(x1)\n",
    "    print(model)\n",
    "    print('output shape:',y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c36f8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[CrossVal] Batch=4, Fold=4, Model=MobileViT, Input=charge, Norm=minmax, Exp=1\n",
      "----------- load charge data -------------\n",
      "test battery id: 4, test data shape: (837, 4, 128) (837, 1)\n",
      "3\n",
      "train battery id: 1, (888, 4, 128) (888, 1)\n",
      "3\n",
      "train battery id: 2, (892, 4, 128) (892, 1)\n",
      "3\n",
      "train battery id: 3, (1166, 4, 128) (1166, 1)\n",
      "3\n",
      "train battery id: 5, (775, 4, 128) (775, 1)\n",
      "3\n",
      "train data shape:  (3721, 4, 128) (3721, 1)\n",
      "-------------  finished !  ---------------\n",
      "valid size: 745\n",
      "epoch=[1/100]  train loss : 0.3439826  valid loss : 0.0276541  lr : 0.0020000\n",
      " ------ test loss : 0.0452687\n",
      "MAE : 0.0144667\n",
      "MAPE : 0.0162444\n",
      "MSE : 0.0005979\n",
      "RMSE : 0.0244521\n",
      "RE : 0.0154023\n",
      "R2 : 0.4870376\n",
      "epoch=[2/100]  train loss : 0.0126370  valid loss : 0.0724782  lr : 0.0020000\n",
      "epoch=[3/100]  train loss : 0.0035466  valid loss : 0.0835459  lr : 0.0020000\n",
      "epoch=[4/100]  train loss : 0.0023662  valid loss : 0.0161182  lr : 0.0020000\n",
      " ------ test loss : 0.0203223\n",
      "MAE : 0.0195304\n",
      "MAPE : 0.0206738\n",
      "MSE : 0.0004175\n",
      "RMSE : 0.0204326\n",
      "RE : 0.0207935\n",
      "R2 : 0.6418197\n",
      "epoch=[5/100]  train loss : 0.0027781  valid loss : 0.0053118  lr : 0.0020000\n",
      " ------ test loss : 0.0061079\n",
      "MAE : 0.0057761\n",
      "MAPE : 0.0063998\n",
      "MSE : 0.0000809\n",
      "RMSE : 0.0089966\n",
      "RE : 0.0061496\n",
      "R2 : 0.9305599\n",
      "epoch=[6/100]  train loss : 0.0034046  valid loss : 0.0592209  lr : 0.0020000\n",
      "epoch=[7/100]  train loss : 0.0023167  valid loss : 0.0091289  lr : 0.0020000\n",
      "epoch=[8/100]  train loss : 0.0015592  valid loss : 0.0465440  lr : 0.0020000\n",
      "epoch=[9/100]  train loss : 0.0015989  valid loss : 0.1406212  lr : 0.0020000\n",
      "epoch=[10/100]  train loss : 0.0020393  valid loss : 0.3080108  lr : 0.0020000\n",
      "\n",
      "epoch=[11/100]  train loss : 0.0025363  valid loss : 0.2005616  lr : 0.0020000\n",
      "epoch=[12/100]  train loss : 0.0022685  valid loss : 0.1092297  lr : 0.0020000\n",
      "epoch=[13/100]  train loss : 0.0021587  valid loss : 0.0871483  lr : 0.0020000\n",
      "epoch=[14/100]  train loss : 0.0018867  valid loss : 0.0060646  lr : 0.0020000\n",
      "epoch=[15/100]  train loss : 0.0023581  valid loss : 0.0824859  lr : 0.0020000\n",
      "epoch=[16/100]  train loss : 0.0020407  valid loss : 0.4473832  lr : 0.0010000\n",
      "epoch=[17/100]  train loss : 0.0014487  valid loss : 0.1044378  lr : 0.0010000\n",
      "epoch=[18/100]  train loss : 0.0013341  valid loss : 0.0058159  lr : 0.0010000\n",
      "epoch=[19/100]  train loss : 0.0011728  valid loss : 0.0293575  lr : 0.0010000\n",
      "epoch=[20/100]  train loss : 0.0014225  valid loss : 0.0221827  lr : 0.0010000\n",
      "\n",
      "epoch=[21/100]  train loss : 0.0015328  valid loss : 0.0447390  lr : 0.0010000\n",
      "epoch=[22/100]  train loss : 0.0017208  valid loss : 0.0464560  lr : 0.0010000\n",
      "epoch=[23/100]  train loss : 0.0013116  valid loss : 0.0287808  lr : 0.0010000\n",
      "epoch=[24/100]  train loss : 0.0014082  valid loss : 0.0424963  lr : 0.0010000\n",
      "epoch=[25/100]  train loss : 0.0012938  valid loss : 0.0031563  lr : 0.0010000\n",
      " ------ test loss : 0.0021259\n",
      "MAE : 0.0059157\n",
      "MAPE : 0.0063384\n",
      "MSE : 0.0000390\n",
      "RMSE : 0.0062477\n",
      "RE : 0.0062983\n",
      "R2 : 0.9665117\n",
      "epoch=[26/100]  train loss : 0.0013516  valid loss : 0.1048722  lr : 0.0010000\n",
      "epoch=[27/100]  train loss : 0.0020330  valid loss : 0.1440677  lr : 0.0010000\n",
      "epoch=[28/100]  train loss : 0.0019200  valid loss : 0.2063083  lr : 0.0010000\n",
      "epoch=[29/100]  train loss : 0.0014096  valid loss : 0.1089682  lr : 0.0010000\n",
      "epoch=[30/100]  train loss : 0.0020924  valid loss : 0.0495756  lr : 0.0010000\n",
      "\n",
      "epoch=[31/100]  train loss : 0.0013129  valid loss : 0.0279782  lr : 0.0010000\n",
      "epoch=[32/100]  train loss : 0.0015951  valid loss : 0.0239765  lr : 0.0010000\n",
      "epoch=[33/100]  train loss : 0.0025367  valid loss : 0.0846965  lr : 0.0010000\n",
      "epoch=[34/100]  train loss : 0.0021698  valid loss : 0.4342774  lr : 0.0010000\n",
      "epoch=[35/100]  train loss : 0.0015782  valid loss : 0.0004378  lr : 0.0010000\n",
      " ------ test loss : 0.0003282\n",
      "MAE : 0.0013933\n",
      "MAPE : 0.0015312\n",
      "MSE : 0.0000045\n",
      "RMSE : 0.0021288\n",
      "RE : 0.0014835\n",
      "R2 : 0.9961122\n",
      "epoch=[36/100]  train loss : 0.0014776  valid loss : 0.0073801  lr : 0.0010000\n",
      "epoch=[37/100]  train loss : 0.0020173  valid loss : 0.1594500  lr : 0.0010000\n",
      "epoch=[38/100]  train loss : 0.0012060  valid loss : 0.1447737  lr : 0.0010000\n",
      "epoch=[39/100]  train loss : 0.0014319  valid loss : 0.0040446  lr : 0.0010000\n",
      "epoch=[40/100]  train loss : 0.0011242  valid loss : 0.0052758  lr : 0.0010000\n",
      "\n",
      "epoch=[41/100]  train loss : 0.0011460  valid loss : 0.0200258  lr : 0.0010000\n",
      "epoch=[42/100]  train loss : 0.0015181  valid loss : 0.0011355  lr : 0.0010000\n",
      "epoch=[43/100]  train loss : 0.0018635  valid loss : 0.3342655  lr : 0.0010000\n",
      "epoch=[44/100]  train loss : 0.0014614  valid loss : 0.0023932  lr : 0.0010000\n",
      "epoch=[45/100]  train loss : 0.0013472  valid loss : 0.0589598  lr : 0.0010000\n",
      "epoch=[46/100]  train loss : 0.0011729  valid loss : 0.0017712  lr : 0.0005000\n",
      "epoch=[47/100]  train loss : 0.0010976  valid loss : 0.0006703  lr : 0.0005000\n",
      "epoch=[48/100]  train loss : 0.0009976  valid loss : 0.0197823  lr : 0.0005000\n",
      "epoch=[49/100]  train loss : 0.0015588  valid loss : 0.0057533  lr : 0.0005000\n",
      "epoch=[50/100]  train loss : 0.0012354  valid loss : 0.4155759  lr : 0.0005000\n",
      "\n",
      "epoch=[51/100]  train loss : 0.0010964  valid loss : 0.0014144  lr : 0.0005000\n",
      "epoch=[52/100]  train loss : 0.0014510  valid loss : 0.0152534  lr : 0.0005000\n",
      "epoch=[53/100]  train loss : 0.0014660  valid loss : 0.1632737  lr : 0.0005000\n",
      "epoch=[54/100]  train loss : 0.0011375  valid loss : 0.0611213  lr : 0.0005000\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='A benchmark for SOH estimation')\n",
    "    parser.add_argument('--random_seed', type=int, default=2023)\n",
    "    # data\n",
    "    parser.add_argument('--data', type=str, default='MIT',choices=['XJTU','MIT'])\n",
    "    parser.add_argument('--input_type',type=str,default='charge',choices=['charge','partial_charge','handcraft_features'])\n",
    "    parser.add_argument('--test_battery_id',type=int,default=1,help='test battery id, 1-8 for XJTU (1-15 for batch-2), 1-5 for MIT')\n",
    "    parser.add_argument('--batch_size',type=int,default=128)\n",
    "    parser.add_argument('--normalized_type',type=str,default='minmax',choices=['minmax','standard'])\n",
    "    parser.add_argument('--minmax_range',type=tuple,default=(-1,1),choices=[(0,1),(-1,1)])\n",
    "    parser.add_argument('--batch', type=int, default=9,choices=[1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "    # model\n",
    "    parser.add_argument('--model',type=str,default='MobileViT',choices=['MobileViT','TransCNN','CNN','LSTM','GRU','MLP','Attention'])\n",
    "    # CNN lr=2e-3  early_stop=30\n",
    "\n",
    "    parser.add_argument('--lr',type=float,default=2e-3)\n",
    "    parser.add_argument('--weight_decay', default=5e-4)\n",
    "    parser.add_argument('--n_epoch',type=int,default=100)\n",
    "    parser.add_argument('--early_stop',default=30)\n",
    "    parser.add_argument('--device',default='cpu')\n",
    "    parser.add_argument('--save_folder',default='results')\n",
    "    parser.add_argument('--experiment_num',default=1,type=int,help='The number of times you want to repeat the same experiment')\n",
    "    parser.add_argument('--test_ids', type=int, nargs='+', default=[1], help='List of test battery IDs to evaluate')\n",
    "\n",
    "    # Use parse_known_args to ignore unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "def load_data(args, test_battery_id):\n",
    "    loader = XJTUDdataset(args) if args.data == 'XJTU' else MITDdataset(args)\n",
    "    if args.input_type == 'charge':\n",
    "        return loader.get_charge_data(test_battery_id)\n",
    "    elif args.input_type == 'partial_charge':\n",
    "        return loader.get_partial_data(test_battery_id)\n",
    "    else:\n",
    "        return loader.get_features(test_battery_id)\n",
    "\n",
    "def run_experiment(args, save_prefix):\n",
    "    try:\n",
    "        data_loader = load_data(args, test_battery_id=args.test_battery_id)\n",
    "        model = SOHMode(args)\n",
    "        model.Train(data_loader['train'], data_loader['valid'], data_loader['test'],\n",
    "                    save_folder=save_prefix)\n",
    "        del model, data_loader\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as ex:\n",
    "        print(f\"[Error] Batch {args.batch}, TestBattery {args.test_battery_id}, Model {args.model}, Input {args.input_type}: {ex}\")\n",
    "        traceback.print_exc()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    \n",
    "def main(args, batches=None, test_ids=None):\n",
    "    batches = batches or [args.batch]\n",
    "    test_ids = test_ids or [args.test_battery_id]\n",
    "\n",
    "    for batch in batches:\n",
    "        for test_id in test_ids:\n",
    "            args.batch = batch\n",
    "            args.test_battery_id = test_id\n",
    "            for e in range(args.experiment_num):\n",
    "                print(f\"\\nRunning: {args.model}, Batch={batch}, TestID={test_id}, Exp={e + 1}\")\n",
    "                save_path = f'results/{args.data}-{args.input_type}/{args.model}/batch{batch}-testbattery{test_id}/experiment{e + 1}'\n",
    "                run_experiment(args, save_path)\n",
    "\n",
    "\n",
    "def multi_task_XJTU():  # 一次性训练所有模型和所有输入类型\n",
    "    args = get_args()\n",
    "    setattr(args,'data','XJTU')\n",
    "    for m in ['MobileViT','TransCNN','CNN','MLP','Attention','LSTM','GRU']:\n",
    "        for type in ['handcraft_features','charge','partial_charge']:\n",
    "            setattr(args, 'model', m)\n",
    "            setattr(args, 'input_type',type)\n",
    "            main(args)\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "def multi_task_MIT():\n",
    "    args = get_args()\n",
    "    for norm in ['standard', 'minmax']:\n",
    "        args.normalized_type = norm\n",
    "        args.minmax_range = (0, 1)\n",
    "        args.data = 'MIT'\n",
    "        for m in ['GRU']:\n",
    "            for input_type in ['partial_charge']:\n",
    "                args.model = m\n",
    "                args.input_type = input_type\n",
    "                for batch in range(1, 10):\n",
    "                    args.batch = batch\n",
    "                    for test_id in [1, 2, 3, 4, 5]:\n",
    "                        args.test_battery_id = test_id\n",
    "                        for e in range(5):\n",
    "                            print(f\"\\n[MIT Run] Model={args.model}, Batch={batch}, TestID={test_id}, Input={input_type}, Norm={norm}, Exp={e + 1}\")\n",
    "                            save_path = f'results/{norm}/{args.data}-{args.input_type}/{args.model}/batch{batch}-testbattery{test_id}/experiment{e + 1}'\n",
    "                            run_experiment(args, save_path)\n",
    "\n",
    "\n",
    "def crossval_mit(\n",
    "    models=['MobileViT'], \n",
    "    input_types=['charge', 'partial_charge', 'handcraft_features'],\n",
    "    normalization_types=['minmax', 'standard'],\n",
    "    test_ids=[1, 2, 3, 4, 5],\n",
    "    batches=range(1, 10),\n",
    "    experiment_num=1\n",
    "):\n",
    "    args = get_args()\n",
    "    args.data = 'MIT'\n",
    "    args.experiment_num = experiment_num\n",
    "\n",
    "    for norm in normalization_types:\n",
    "        args.normalized_type = norm\n",
    "        args.minmax_range = (-1, 1) if norm == 'minmax' else (0, 1)\n",
    "\n",
    "        for batch in batches:\n",
    "            for test_id in test_ids:\n",
    "                for model_name in models:\n",
    "                    for input_type in input_types:\n",
    "                        for e in range(experiment_num):\n",
    "                            args_copy = deepcopy(args)\n",
    "                            args_copy.batch = batch\n",
    "                            args_copy.test_battery_id = test_id\n",
    "                            args_copy.model = model_name\n",
    "                            args_copy.input_type = input_type\n",
    "\n",
    "                            print(f\"\\n[CrossVal] Batch={batch}, Fold={test_id}, Model={model_name}, \"\n",
    "                                  f\"Input={input_type}, Norm={norm}, Exp={e+1}\")\n",
    "                            \n",
    "                            save_path = f'results/crossval/{norm}/{args_copy.data}-{input_type}/{model_name}/batch{batch}-fold{test_id}/experiment{e+1}'\n",
    "                            run_experiment(args_copy, save_path)\n",
    "\n",
    "                          \n",
    "                            \n",
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "\n",
    "#     from argparse import ArgumentParse\n",
    "\n",
    "#     parser = ArgumentParser()\n",
    "#     parser.add_argument('--task', type=str, default='crossval_mit', choices=[\n",
    "#         'single', 'multi_xjtu', 'multi_mit', 'crossval_mit'\n",
    "#     ])\n",
    "#     task_args, _ = parser.parse_known_args()\n",
    "    task = 'crossval_mit'\n",
    "\n",
    "    if task == 'single':\n",
    "        for e in range(args.experiment_num):\n",
    "            print(f\"\\n[Single Run] {args.model} on Battery {args.test_battery_id}, Batch {args.batch}, Input={args.input_type}\")\n",
    "            save_path = f'results/{args.data}-{args.input_type}/{args.model}/batch{args.batch}-testbattery{args.test_battery_id}/experiment{e + 1}'\n",
    "            run_experiment(args, save_path)\n",
    "\n",
    "    elif task == 'multi_xjtu':\n",
    "        multi_task_XJTU()\n",
    "\n",
    "    elif task == 'multi_mit':\n",
    "        multi_task_MIT()\n",
    "\n",
    "    elif task == 'crossval_mit':\n",
    "        crossval_mit(\n",
    "        models=['MobileViT'],\n",
    "        input_types=['charge'],\n",
    "        normalization_types=['minmax'],\n",
    "        test_ids=[4],\n",
    "        batches=[4],\n",
    "        experiment_num=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c672569d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
